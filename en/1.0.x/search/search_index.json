{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architecture-details/","title":"How the solution works","text":"<p>The Clickstream Analytics on AWS solution has three components: a web console, SDKs, and data pipeline.</p>"},{"location":"architecture-details/#web-console","title":"Web console","text":"<p>This solution provides a web console which allows you to create clickstream projects, and configure, deploy, and manage  data pipeline for each clickstream project.</p>"},{"location":"architecture-details/#sdks","title":"SDKs","text":"<p>This solution provides native SDKs for help you easily collect and report in-app events from your applications to data pipelines.</p> <ul> <li>Android SDK</li> <li>Swift SDK</li> </ul>"},{"location":"architecture-details/#data-pipeline","title":"Data pipeline","text":"<p>This solution uses the web console to manage the project and its data pipeline. The data pipeline consists of four modules.</p>"},{"location":"architecture-details/#ingestion-module","title":"Ingestion module","text":"<p>The ingestion module serves as web server for ingesting the Clickstream data. It supports the following features:</p> <ul> <li>specify the auto scaling group capability</li> <li>specify warm pool size to scale out faster and save costs</li> <li>support authenticate with OIDC</li> <li>support SSL</li> <li>support enabling AWS Global Accelerator for ELB</li> <li>support different data sinks, including S3, KDS and MSK</li> </ul>"},{"location":"architecture-details/#data-processing-module","title":"Data processing module","text":"<p>The data processing module transforms and enriches the ingested data to solution's data model by the Apache Spark application running in EMR serverless. It supports the following features:</p> <ul> <li>specify the batch interval of data processing</li> <li>specify the data refreshness age</li> <li>provider out-of-the-box enrichment plug-ins</li> <li>UA enrichment to parse OS, device, browser information from User Agent string of the HTTP request header</li> <li>IP enrichment to mapping device location information (for example, city, country, region) based on the request source IP</li> <li>support third-party transformer plug-ins</li> <li>support third-party enrichment plug-ins</li> </ul>"},{"location":"architecture-details/#data-modeling-module","title":"Data modeling module","text":"<p>The data modeling module loads the processed data into data lakehouse. It supports the following features:</p> <ul> <li>support both provisioned Redshift and Redshift Serverless as data warehouse</li> <li>support the data range for hot data keeping in Redshift</li> <li>specify the interval to update user dimension table</li> <li>support use Athena to query the data in data lake</li> </ul>"},{"location":"architecture-details/#reporting-module","title":"Reporting module","text":"<p>The reporting module queries the data in Redshift with out-of-the-box Clickstream reports.</p>"},{"location":"architecture/","title":"Architecture diagram","text":""},{"location":"architecture/#solution-end-to-end-architecture","title":"Solution end-to-end architecture","text":"<p>Deploying this solution with the default parameters builds the following environment in AWS:</p> <p> </p> Figure 1: Clickstream Analytics on AWS architecture <p>This solution deploys the Amazon CloudFormation template in your AWS account and completes the following settings.</p> <ol> <li>Amazon CloudFront distributes the frontend web UI assets hosted in the Amazon S3 bucket, and the backend APIs hosted with Amazon API Gateway and AWS Lambda.</li> <li>The Amazon Cognito user pool or OpenID Connect (OIDC) is used for authentication.</li> <li>The web UI console uses Amazon DynamoDB to store persistent data.</li> <li>AWS Step Functions, AWS CloudFormation, AWS Lambda, and Amazon EventBridge are used for orchestrating the lifecycle management of data pipelines.</li> <li>The data pipeline is provisioned in the region specified by the system operator. It consists of Application Load Balancer (ALB), Amazon ECS, Amazon Managed Streaming for Kafka (Amazon MSK), Amazon Kinesis Data Streams, Amazon S3, Amazon EMR Serverless, Amazon Redshift, and Amazon QuickSight.</li> </ol> <p>The key functionality of this solution is to build a data pipeline to collect, process, and analyze their clickstream data. The data pipeline consists of four modules: </p> <ul> <li>ingestion module </li> <li>data processing module </li> <li>data modeling module </li> <li>reporting module </li> </ul> <p>The following introduces the architecture diagram for each module.</p>"},{"location":"architecture/#ingestion-module","title":"Ingestion module","text":"Figure 2: Ingestion module architecture <p>Suppose you create a data pipeline in the solution. This solution deploys the Amazon CloudFormation template in your AWS account and completes the following settings.</p> <p>Note</p> <p>The ingestion module supports three types of data sinks.</p> <ol> <li>(Optional) The ingestion module creates an AWS global accelerator endpoint to reduce the latency of sending events from your clients (web applications or mobile applications).</li> <li>Elastic Load Balancing (ELB) is used for load balancing ingestion web servers.</li> <li>(Optional) If you enable the authenticating feature, the ALB will communicate with the OIDC provider to authenticate the requests.</li> <li>ALB forwards all authenticated and valid requests to the ingestion servers.</li> <li>Amazon ECS cluster is hosting the ingestion fleet servers. Each server consists of a proxy and a worker service. The proxy is a facade of the HTTP protocol, and the worker will send the events to a data sink based on your choice.</li> <li>Amazon Kinesis Data Streams is used as a buffer. AWS Lambda consumes the events in Kinesis Data Streams and then sinks them to Amazon S3 in batches.</li> <li>Amazon MSK or self-built Kafka is used as a buffer. MSK Connector is provisioned with an S3 connector plugin that sinks the events to Amazon S3 in batches.</li> <li>The ingestion server will buffer a batch of events and sink them to Amazon S3.</li> </ol>"},{"location":"architecture/#data-processing-module","title":"Data processing module","text":"Figure 3: Data processing module architecture <p>Suppose you create a data pipeline in the solution and enable ETL. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>Amazon EventBridge is used to trigger the ETL jobs periodically.</li> <li>The configurable time-based scheduler invokes an AWS Lambda function.</li> <li>The Lambda function kicks off an EMR Serverless application based on Spark to process a batch of clickstream events.</li> <li>The EMR Serverless application uses the configurable transformer and enrichment plug-ins to process the clickstream events from the source S3 bucket.</li> <li>After processing the clickstream events, the EMR Serverless application sinks the processed events to the sink S3 bucket.</li> </ol>"},{"location":"architecture/#data-modeling-module","title":"Data modeling module","text":"Figure 4: Data modeling in Redshift architecture <p>Suppose you create a data pipeline in the solution and enable data modeling in Amazon Redshift. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>After the processed clickstream events data is written in the Amazon S3 bucket, the <code>Object Created Event</code> is emitted.</li> <li>An Amazon EventBridge rule is created for the event emitted in step 1, and an AWS Lambda function is invoked when the event happens.</li> <li>The Lambda function persists the source event to be loaded in an Amazon DynamoDB table.</li> <li>When data processing job is done, an event is emitted to Amazon EventBridge.</li> <li>The pre-defined event rule of Amazon EventBridge processes the <code>EMR job success event</code>.</li> <li>The rule invokes the AWS Step Functions workflow.</li> <li>The workflow invokes the <code>list objects</code> Lambda function that queries the DynamoDB table to find out the data to be loaded, then creates a manifest file for a batch of event data to optimize the load performance.</li> <li>After a few seconds, the <code>check status</code> Lambda function starts to check the status of loading job.</li> <li>If the load is still in progress, the <code>check status</code> Lambda function waits a few more seconds.</li> <li>After all objects are loaded, the workflow ends.</li> </ol> <p> </p> Figure 5: Data modeling in Athena architecture <p>Suppose you create a data pipeline in the solution and enable data modeling in Amazon Athena. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>Amazon EventBridge initiates the data load into Amazon Athena periodically.</li> <li>The configurable time-based scheduler invokes an AWS Lambda function.</li> <li>The AWS Lambda function creates the partitions of the AWS Glue table for the processed clickstream data.</li> <li>Amazon Athena is used for interactive querying of clickstream events.</li> <li>The processed clickstream data is scanned via the Glue table.</li> </ol>"},{"location":"architecture/#reporting-module","title":"Reporting module","text":"Figure 6: Reporting module architecture <p>Suppose you create a data pipeline in the solution, enable data modeling in Amazon Redshift, and enable reporting in Amazon QuickSight. This solution deploys the Amazon CloudFormation template in your AWS Cloud account and completes the following settings.</p> <ol> <li>VPC connection in Amazon QuickSight is used for securely connecting your Redshift within VPC.</li> <li>The data source, data sets, template, analysis, and dashboard are created in Amazon QuickSight for out-of-the-box analysis and visualization.</li> </ol>"},{"location":"aws-services/","title":"AWS services in this solution","text":"<p>This section describes the components and AWS services that make up this solution and the architecture details on how these components work together.</p>"},{"location":"aws-services/#aws-services-in-this-solution","title":"AWS services in this solution","text":"<p>The following AWS services are included in this solution:</p> AWS service Description Amazon Elastic Load Balancing Core.  To distribute network traffic to ingestion fleet. Amazon ECS Core.  To run the ingestion module fleet. Amazon EC2 Core. To provide the underlying computing resources for ingestion fleet. Amazon ECR Core. To host the container images used by ingestion fleet. Amazon S3 Core. To store the ingested and processed Clickstream data. And it also stores the service logs and static web assets (frontend user interface). AWS Global Accelerator Supporting. To improve the availability, performance, and security of the ingestion service in AWS Regions. AWS CloudWatch Supporting. To monitor the metrics, logs and trace of data pipeline. Amazon SNS Supporting. To provide topic and email subscription notifications for the alarms of data pipeline. Amazon Kinesis Data Streams Supporting. To provide the ingestion buffer. AWS Lambda Supporting. To integrate with kinds of AWS services. For example, sink ingestion data to S3, manage the lifecycle of AWS resources. Amazon Managed Streaming for Apache Kafka (MSK) Supporting. To provide the ingestion buffer with Apache Kafka. Amazon EMR Serverless Supporting. To process the ingested data. Amazon Glue Supporting. To manage the data catalog of ingested data. Amazon EventBridge Supporting. To integrate with AWS services with events or schedule. Amazon Redshift Supporting. To analyze your Clickstream data in data warehouse. Amazon Athena Supporting. To analyze your Clickstream data in data lake. AWS Step Functions Supporting. To orchestrate the lifecycle management of project's pipeline. Also it manages the workflow to load data into data warehouse. AWS Secrets Manager Supporting. To store the credential for OIDC credentials and BI user in Redshift. Amazon QuickSight Supporting. Visual your analysis reporting of your Clickstream data. Amazon CloudFront Supporting. To made available the static web assets (frontend user interface) and proxy the backend in the same origin. Amazon Cognito Supporting. To authenticate users (in AWS Regions). Amazon API Gateway Supporting. To provide the backend APIs. Amazon DynamoDB Supporting.  To store projects data. AWS CloudFormation Supporting.  To provision the AWS resources for the modules of data pipeline."},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":"<p>Q:  What is Clickstream Analytics on AWS? An AWS Solution that enables customers to build clickstream analytic system on AWS easily. This solution automates the data pipeline creation per customers\u2019 configurations with a visual pipeline builder, and provides SDKs for web and mobiles apps (including iOS, Android, and Web JS) to help customers to collect and ingest client-side data into the data pipeline on AWS. After data ingestion, the solution allows customers to further enrich and model the event data for business users to query, and provides built-in visualizations (e.g., acquisition, engagement, retention) to help them generate insights faster.</p>"},{"location":"faq/#sdk","title":"SDK","text":"<p>Q: Can I use other SDK to send data to the pipeline created by this solution Yes, you can. The solution support users using third-party SDK to send data to the pipeline. Note that, if you want to enable data processing and modeling module when using a third-party SDK to send data, you will need to provide an transformation plugin to map third-party SDK's data structure to solution data schema. Please refer to Custom plugin for more details.</p>"},{"location":"faq/#setup-and-configuration","title":"Setup and configuration","text":"<p>Q: How can I create more users for this solution? If you launched the solution with Cognito User Pool, go to the AWS console, find the user pool created by the solution, and you can create more users. If you launched the solution with OpenID Connect (OIDC), you should add more users in the user pool managed by the OIDC provider. Note that all users have the same privileges. </p>"},{"location":"faq/#pricing","title":"Pricing","text":"<p>Q: How will I be charged and billed for the use of this solution? The solution is free to use, and you are responsible for the cost of AWS services used while running this solution.  You pay only for what you use, and there are no minimum or setup fees. Refer to the Cost section for detailed cost estimation. </p>"},{"location":"notices/","title":"Notices","text":"<p>Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers.</p> <p>Clickstream Analytics on AWS is licensed under the terms of the Apache License Version 2.0 available at The Apache Software Foundation.</p>"},{"location":"revisions/","title":"Revisions","text":"Date Change July 2023 Initial release"},{"location":"source/","title":"Source code","text":"<p>Visit our GitHub repository to download the source code for this solution. The Clickstream Analytics on AWS template is generated using the AWS Cloud Development Kit (CDK). Refer to the README.md file for additional information.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>The following help you to fix errors or problems that you might encounter when using Clickstream Analytics on AWS.</p>"},{"location":"troubleshooting/#problem-deployment-failure-due-to-invalid-logging-configuration-the-cloudwatch-logs-resource-policy-size-was-exceeded","title":"Problem: Deployment failure due to \"Invalid Logging Configuration: The CloudWatch Logs Resource Policy size was exceeded\"","text":"<p>If you encounter a deployment failure due to creating CloudWatch log group with an error message like the one below,</p> <p>Cannot enable logging. Policy document length breaking Cloudwatch Logs Constraints, either &lt; 1 or &gt; 5120 (Service: AmazonApiGatewayV2; Status Code: 400; Error Code: BadRequestException; Request ID: xxx-yyy-zzz; Proxy: null)</p> <p>Resolution:</p> <p>CloudWatch Logs resource policies are limited to 5120 characters. The remediation is merging or removing useless policies, then updating the resource policies of CloudWatch logs to reduce the number of policies.</p> <p>Below is a sample command to reset resource policy of CloudWatch logs:</p> <pre><code>aws logs put-resource-policy --policy-name AWSLogDeliveryWrite20150319 \\\n--policy-document '\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AWSLogDeliveryWrite2\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"delivery.logs.amazonaws.com\"\n      },\n      \"Action\": [\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:SourceAccount\": \"&lt;your AWS account id&gt;\"\n        },\n        \"ArnLike\": {\n          \"aws:SourceArn\": \"arn:aws:logs:&lt;AWS region&gt;:&lt;your AWS account id&gt;:*\"\n        }\n      }\n    }\n  ]\n}\n'\n</code></pre>"},{"location":"troubleshooting/#problem-can-not-delete-the-cloudformation-stacks-created-for-the-clickstream-pipeline","title":"Problem: Can not delete the CloudFormation stacks created for the Clickstream pipeline","text":"<p>If you encounter a failure with an error message like the one below when deleting the CloudFormation stacks created for the Clickstream pipeline,</p> <p>Role arn:aws:iam::&lt;your AWS account id&gt;:role/&lt;stack nam&gt;-ClickStreamApiStackActionSta-&lt;random suffix&gt; is invalid or cannot be assumed</p> <p>Resolution:</p> <p>It results from deleting the web console stack for this solution before the CloudFormation stacks are made for the Clickstream pipeline.</p> <p>Please create a new IAM role with the identical name mentioned in the above error message and trust the CloudFormation service with sufficient permission to delete those stacks.</p> <p>Tip</p> <p>You can delete the IAM role after successfully removing those CloudFormation stacks.</p>"},{"location":"troubleshooting/#problem-can-not-sink-data-to-msk-cluster-got-invalidreplicationfactor-broker-invalid-replication-factor-log-in-ingestion-server","title":"Problem: Can not sink data to MSK cluster, got \"InvalidReplicationFactor (Broker: Invalid replication factor)\" log in Ingestion Server","text":"<p>If you notice that data can not be sunk into S3 through MSK cluster, and the error message in log of Ingestion Server (ECS) worker task is as below:</p> <p>Message production error: InvalidReplicationFactor (Broker: Invalid replication factor)</p> <p>Resolution:</p> <p>This is caused by replication factor larger than available brokers, please edit the MSK cluster configuration, set default.replication.factor not larger than the total number of brokers.</p>"},{"location":"troubleshooting/#problem-data-processing-job-failure","title":"Problem: data processing job failure","text":"<p>If the data processing job implemented by EMR serverless fails with the below errors:</p> <ul> <li> <p>IOException: No space left on device</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: Caused by: java.io.IOException: No space left on device Exception in thread \"main\" org.apache.spark.SparkException:</p> </li> <li> <p>ExecutorDeadException</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: Caused by: org.apache.spark.ExecutorDeadException: The relative remote executor(Id: 34), which maintains the block data to fetch is dead. org.apache.spark.shuffle.FetchFailedException Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage</p> </li> <li> <p>Could not find CoarseGrainedScheduler</p> <p>Job failed, please check complete logs in configured logging destination. ExitCode: 1. Last few exceptions: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.</p> </li> </ul> <p>You need to tune the EMR job default configuration, and please refer to the configure execution parameters.</p>"},{"location":"uninstall/","title":"Uninstall the Clickstream Analytics on AWS","text":"<p>Warning</p> <p>You will encounter an IAM role missing error if you delete Clickstream Analytics on AWS main stack before you delete the stacks created for Clickstream projects. Clickstream Analytics on AWS console launches additional CloudFormation stacks for the Clickstream pipelines. We recommend you delete projects before uninstalling the solution.</p>"},{"location":"uninstall/#step-1-delete-projects","title":"Step 1. Delete projects","text":"<ol> <li>Go to the Clickstream Analytics on AWS console. </li> <li>In the left sidebar, choose Projects.</li> <li>Select the project to be deleted.</li> <li>Choose the Delete button in the upper right corner.</li> <li>Repeat steps 3 and 4 to delete all your projects.</li> </ol>"},{"location":"uninstall/#step-2-delete-clickstream-analytics-on-aws-stack","title":"Step 2. Delete Clickstream Analytics on AWS stack","text":"<ol> <li>Go to the CloudFormation console.</li> <li>Find the CloudFormation stack of the solution.</li> <li>Delete the CloudFormation Stack of the solution.</li> <li>(Optional) Delete the S3 bucket created by the solution.<ol> <li>Choose the CloudFormation stack of the solution, and select the Resources tab.</li> <li>In the search bar, enter <code>DataBucket</code>. It shows all resources with the name <code>DataBucket</code> created by the solution. You can find the resource type AWS::S3::Bucket, and the Physical ID field is the S3 bucket name.</li> <li>Go to the S3 console, and find the S3 bucket with the bucket name. Empty and Delete the S3 bucket.</li> </ol> </li> </ol>"},{"location":"well-architected-pillars/","title":"AWS Well-Architected pillars","text":"<p>This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud.</p> <p>This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution.</p>"},{"location":"well-architected-pillars/#operational-excellence","title":"Operational excellence","text":"<p>This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution.</p> <p>The Clickstream Analytics on AWS solution pushes metrics, logs and traces to Amazon CloudWatch at various stages to provide observability into the infrastructure, Elastic load balancer, Amazon ECS cluster, Lambda functions, EMR serverless application, Step Function workflow and the rest of the solution components. This solution also creates the CloudWatch dashboard for each data pipeline.</p>"},{"location":"well-architected-pillars/#security","title":"Security","text":"<p>This section describes how the principles and best practices of the security pillar were applied when designing this solution.</p> <ul> <li>Clickstream Analytics on AWS web console users are authenticated and authorized with Amazon Cognito or OpenID Connect.</li> <li>All inter-service communications use AWS IAM roles.</li> <li>All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly.</li> </ul>"},{"location":"well-architected-pillars/#reliability","title":"Reliability","text":"<p>This section describes how the principles and best practices of the reliability pillar were applied when designing this solution.</p> <ul> <li>Using AWS serverless services wherever possible (for example, EMR Serverless, Redshift Serverless, Lambda, Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure.</li> <li>Data ingested by data pipeline is stored in Amazon S3 and Amazon Redshift, so it persists in multiple Availability Zones (AZs) by default.</li> </ul>"},{"location":"well-architected-pillars/#performance-efficiency","title":"Performance efficiency","text":"<p>This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution.</p> <ul> <li>The ability to launch this solution in any Region that supports AWS services in this solution such as: Amazon S3, Amazon ECS, Elastic load balancer.</li> <li>Using serverless architecture removes the need for you to run and maintain physical servers for traditional compute activities.</li> <li>Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve.</li> </ul>"},{"location":"well-architected-pillars/#cost-optimization","title":"Cost optimization","text":"<p>This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution.</p> <ul> <li>Use Autoscaling Group so that the compute costs are only related to how much data is ingested and processed.</li> <li>Using serverless services such as Amazon S3, Amazon Kinesis Data Streams, Amazon EMR Serverless and Amazon Redshift Serverless so that customers only get charged for what they use.</li> </ul>"},{"location":"well-architected-pillars/#sustainability","title":"Sustainability","text":"<p>This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution.</p> <ul> <li>The solution\u2018s serverless design (using Amazon Kinesis Data Streams, Amazon EMR Serverless, Amazon Redshift Serverless and Amazon QuickSight) and the use of managed services (such as Amazon ECS, Amazon MSK) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.</li> </ul>"},{"location":"dashboard/","title":"Dashboard","text":"<p>Clickstream Analytics on AWS solution collects data from your websites and apps to create dashboards that derive insights. You can use dashboards to monitor traffic, investigate data, and understand your users and their activities.</p> <p>Once the data are processed by the data pipeline, the data appears in the QuickSight dashboards. Depends on your pipeline configuration, the time for data to be available in your dashboard varies. For example, if you set the data processing interval to be 1 day, the dashboard will show data at T+1 day (T as reporting date).</p>"},{"location":"dashboard/#view-dashboards","title":"View dashboards","text":"<p>You can find the dashboards for each application by following below steps:</p> <ol> <li>Go to Clickstream Analytics on AWS Console, in the Navigation Bar, click on \"Project\", then click the project you want to view dashboards.</li> <li>In the project detail page, click on <code>pipeline ID</code> or View Details button, it will bring you to the pipeline detail page.</li> <li>In the pipeline details page, select the tab of Reporting, you will see the link to the dashboards created for your app.</li> <li>Click on the dashboard link with the name of your app, it will bring you to the QuickSight dashboard. </li> <li>Click on the dashboard with name start with <code>Clickstream</code>.</li> </ol>"},{"location":"dashboard/#reports","title":"Reports","text":"<p>The dashboards contains a set of reports covering the following topics.</p> Report name What it is Acquisition Summarizes key metrics about new users, provides detail view of user profile Engagement Summarizes key metrics about user engagements and sessions Activity Summarizes key metrics about events user generates in the app, provide detail view of event attributes Retention Summarizes key metrics about active users and user retentions Device Summarizes key metrics about the devices users are using to access your apps and websites, provides detail view of each device Path explorer Provides charts for you to understand user journey in your apps and websites"},{"location":"dashboard/#custom-report","title":"Custom report","text":"<p>When you want to investigate certain pieces of data further, you can write SQL to create views in Redshift or Athena, then add dataset into QuickSight to create visualization. Here is an example, Custom report, to demonstrate how to create a customize report with Redshift. </p>"},{"location":"dashboard/acquisition/","title":"Acquisition report","text":"<p>You can use the User acquisition report to get insights into how new users find your website or app for the first time. This report also allows you view the detail user profile.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/acquisition/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Acquisition</code>.</li> </ol>"},{"location":"dashboard/acquisition/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Acquisistion report are created based on the QuickSight dataset of <code>user_dim_view-&lt;app&gt;-&lt;project&gt;</code>, which connects to the <code>clickstream_user_dim_view</code> view in analytics engine (i.e., Redshift or Athena). Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-user-dim-view.sql<pre><code>SELECT\nuser_pseudo_id\n, event_date as first_visit_date\n, app_info.install_source::varchar as first_visit_install_source\n, device.system_language::varchar as first_visit_device_language\n, platform as first_platform\n, geo.country::varchar as first_visit_country\n, geo.city::varchar as first_visit_city\n, (case when nullif(traffic_source.source::varchar,'') is null then '(direct)' else traffic_source.source::varchar end) as first_traffic_source_source\n, traffic_source.medium::varchar as first_traffic_source_medium\n, traffic_source.name::varchar as first_traffic_source_name\nfrom {{schema}}.ods_events\nwhere event_name in ('_first_open','_first_visit');\n-- recompute refresh\nCREATE MATERIALIZED VIEW {{schema}}.clickstream_user_dim_mv_2 BACKUP NO\nSORTKEY(user_pseudo_id)\nAUTO REFRESH YES\nAS\nselect user_pseudo_id,\ncount\n(\ndistinct user_id\n) as user_id_count\nfrom {{schema}}.ods_events ods\nwhere event_name not in (\n'_first_open',\n'_first_visit'\n) group by 1\n;\nCREATE OR REPLACE VIEW {{schema}}.clickstream_user_dim_view AS\nSELECT upid.*,\n(\ncase when uid.user_id_count&gt;0 then 'Registered' else 'Non-registered' end\n) as is_registered\nfrom {{schema}}.clickstream_user_dim_mv_1 as upid left outer join {{schema}}.clickstream_user_dim_mv_2 as uid on upid.user_pseudo_id=uid.user_pseudo_id;\n</code></pre> clickstream-user-dim-query.sql<pre><code>with base as (\nselect *\nfrom {{database}}.{{eventTable}}\nwhere partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\n),\nclickstream_user_dim_mv_1 as (\nSELECT\nuser_pseudo_id\n, event_date as first_visit_date\n, app_info.install_source as first_visit_install_source\n, device.system_language as first_visit_device_language\n, platform as first_platform\n, geo.country as first_visit_country\n, geo.city as first_visit_city\n, (case when nullif(traffic_source.source,'') is null then '(direct)' else traffic_source.source end) as first_traffic_source_source\n, traffic_source.medium as first_traffic_source_medium\n, traffic_source.name as first_traffic_source_name\nfrom base\nwhere event_name in ('_first_open','_first_visit')\n),\nclickstream_user_dim_mv_2 AS (\nselect user_pseudo_id,\ncount\n(\ndistinct user_id\n) as user_id_count\nfrom base ods\nwhere event_name not in (\n'_first_open',\n'_first_visit'\n) group by 1\n)\nSELECT upid.*,\n(\ncase when uid.user_id_count&gt;0 then 'Registered' else 'Non-registered' end\n) as is_registered\nfrom clickstream_user_dim_mv_1 as upid left outer join clickstream_user_dim_mv_2 as uid on upid.user_pseudo_id=uid.user_pseudo_id\n</code></pre>"},{"location":"dashboard/acquisition/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>first_visit_date</code> Dimension The date that the user first visited your website or first opened the app Query from analytics engine <code>first_install_source</code> Dimension The installation source when user first opened your app. Blank for web Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_visit_device_language</code> Dimension The system language of the device user used when they first opened your app or first visited your website. Query from analytics engine <code>first_platform</code> Dimension The platform when user first visited your website or first opened your app Query from analytics engine <code>first_visit_country</code> Dimension The country where user first visited your website or first opened your app. Query from analytics engine <code>first_visit_city</code> Dimension The city where user first visited your website or first opened your app. Query from analytics engine <code>custom_attr_key</code> Dimension The name of the custom attribute key of the user. Query from analytics engine <code>custom_attr_value</code> Dimension The value of the custom attribute key of the user. Query from analytics engine <code>is_registered</code> Dimension If user had registed or not Query from analytics engine"},{"location":"dashboard/acquisition/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"dashboard/activity/","title":"Activity report","text":"<p>You can use the Activity report to get insights into the activities the users performed when using your websites and apps. This report measures user activity by the events that users triggered, and let you view the detail attributes of the events.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/activity/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Activity</code>.</li> </ol>"},{"location":"dashboard/activity/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Activity report are created based on the following QuickSight datasets:</p> <ul> <li><code>ods_events_view-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_ods_event_rt__view</code> view in analytics engines (i.e., Redshift or Athena)</li> <li><code>ods_events_parameter_view-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_ods_events_parameter_rt_view</code> view in analytics engines  </li> </ul> <p>Below is the SQL command that generates the related views.</p> SQL Commands RedshiftAthena clickstream-ods-events-rt-view.sql<pre><code>SELECT event_date,\nevent_name,\nevent_id,\nevent_bundle_sequence_id,\nevent_previous_timestamp,\nevent_server_timestamp_offset,\nevent_timestamp,\ningest_timestamp,\nevent_value_in_usd,\napp_info_app_id,\napp_info_package_id,\napp_info_install_source,\napp_info_version,\ndevice_id,\ndevice_mobile_brand_name,\ndevice_mobile_model_name,\ndevice_manufacturer,\ndevice_screen_width,\ndevice_screen_height,\ndevice_carrier,\ndevice_network_type,\ndevice_operating_system,\ndevice_operating_system_version,\nua_browser,\nua_browser_version,\nua_os,\nua_os_version,\nua_device,\nua_device_category,\ndevice_system_language,\ndevice_time_zone_offset_seconds,\ngeo_continent,\ngeo_country,\ngeo_city,\ngeo_metro,\ngeo_region,\ngeo_sub_continent,\ngeo_locale,\nplatform,\nproject_id,\ntraffic_source_name,\ntraffic_source_medium,\ntraffic_source_source,\nuser_first_touch_timestamp,\nuser_id,\nuser_pseudo_id\nFROM {{schema}}.clickstream_ods_events_view\nWHERE EXISTS (SELECT 1 FROM {{schema}}.clickstream_ods_events_view LIMIT 1)\nUNION ALL\nSELECT event_date,\nevent_name as event_name,\nevent_id as event_id,\nevent_bundle_sequence_id:: bigint as event_bundle_sequence_id,\nevent_previous_timestamp:: bigint as event_previous_timestamp,\nevent_server_timestamp_offset:: bigint as event_server_timestamp_offset,\nevent_timestamp as event_timestamp,\ningest_timestamp as ingest_timestamp,\nevent_value_in_usd as event_value_in_usd,\napp_info.app_id:: varchar as app_info_app_id,\napp_info.id:: varchar as app_info_package_id,\napp_info.install_source:: varchar as app_info_install_source,\napp_info.version:: varchar as app_info_version,\ndevice.vendor_id::varchar as device_id,\ndevice.mobile_brand_name:: varchar as device_mobile_brand_name,\ndevice.mobile_model_name:: varchar as device_mobile_model_name,\ndevice.manufacturer:: varchar as device_manufacturer,\ndevice.screen_width:: bigint as device_screen_width,\ndevice.screen_height:: bigint as device_screen_height,\ndevice.carrier:: varchar as device_carrier,\ndevice.network_type:: varchar as device_network_type,\ndevice.operating_system:: varchar as device_operating_system,\ndevice.operating_system_version:: varchar as device_operating_system_version,\ndevice.ua_browser:: varchar,\ndevice.ua_browser_version:: varchar,\ndevice.ua_os:: varchar,\ndevice.ua_os_version:: varchar,\ndevice.ua_device:: varchar,\ndevice.ua_device_category:: varchar,\ndevice.system_language:: varchar as device_system_language,\ndevice.time_zone_offset_seconds:: bigint as device_time_zone_offset_seconds,\ngeo.continent:: varchar as geo_continent,\ngeo.country:: varchar as geo_country,\ngeo.city:: varchar as geo_city,\ngeo.metro:: varchar as geo_metro,\ngeo.region:: varchar as geo_region,\ngeo.sub_continent:: varchar as geo_sub_continent,\ngeo.locale:: varchar as geo_locale,\nplatform as platform,\nproject_id as project_id,\ntraffic_source.name:: varchar as traffic_source_name,\ntraffic_source.medium:: varchar as traffic_source_medium,\ntraffic_source.source:: varchar as traffic_source_source,\nuser_first_touch_timestamp as user_first_touch_timestamp,\nuser_id as user_id,\nuser_pseudo_id\nFROM {{schema}}.ods_events e\nWHERE e.event_date &gt;= (SELECT max(event_date) FROM {{schema}}.clickstream_ods_events_view)\nAND e.event_timestamp &gt; (SELECT max(event_timestamp) FROM {{schema}}.clickstream_ods_events_view)\nAND EXISTS (SELECT 1 FROM {{schema}}.clickstream_ods_events_view LIMIT 1)\nUNION ALL\nSELECT event_date,\nevent_name as event_name,\nevent_id as event_id,\nevent_bundle_sequence_id:: bigint as event_bundle_sequence_id,\nevent_previous_timestamp:: bigint as event_previous_timestamp,\nevent_server_timestamp_offset:: bigint as event_server_timestamp_offset,\nevent_timestamp as event_timestamp,\ningest_timestamp as ingest_timestamp,\nevent_value_in_usd as event_value_in_usd,\napp_info.app_id:: varchar as app_info_app_id,\napp_info.id:: varchar as app_info_package_id,\napp_info.install_source:: varchar as app_info_install_source,\napp_info.version:: varchar as app_info_version,\ndevice.vendor_id::varchar as device_id,\ndevice.mobile_brand_name:: varchar as device_mobile_brand_name,\ndevice.mobile_model_name:: varchar as device_mobile_model_name,\ndevice.manufacturer:: varchar as device_manufacturer,\ndevice.screen_width:: bigint as device_screen_width,\ndevice.screen_height:: bigint as device_screen_height,\ndevice.carrier:: varchar as device_carrier,\ndevice.network_type:: varchar as device_network_type,\ndevice.operating_system:: varchar as device_operating_system,\ndevice.operating_system_version:: varchar as device_operating_system_version,\ndevice.ua_browser:: varchar,\ndevice.ua_browser_version:: varchar,\ndevice.ua_os:: varchar,\ndevice.ua_os_version:: varchar,\ndevice.ua_device:: varchar,\ndevice.ua_device_category:: varchar,\ndevice.system_language:: varchar as device_system_language,\ndevice.time_zone_offset_seconds:: bigint as device_time_zone_offset_seconds,\ngeo.continent:: varchar as geo_continent,\ngeo.country:: varchar as geo_country,\ngeo.city:: varchar as geo_city,\ngeo.metro:: varchar as geo_metro,\ngeo.region:: varchar as geo_region,\ngeo.sub_continent:: varchar as geo_sub_continent,\ngeo.locale:: varchar as geo_locale,\nplatform as platform,\nproject_id as project_id,\ntraffic_source.name:: varchar as traffic_source_name,\ntraffic_source.medium:: varchar as traffic_source_medium,\ntraffic_source.source:: varchar as traffic_source_source,\nuser_first_touch_timestamp as user_first_touch_timestamp,\nuser_id as user_id,\nuser_pseudo_id\nFROM {{schema}}.ods_events\nWHERE NOT EXISTS (SELECT 1 FROM {{schema}}.clickstream_ods_events_view LIMIT 1);\n</code></pre> clickstream-ods-events-query.sql<pre><code>select event_date\n,event_name as event_name\n,event_id as event_id\n,event_bundle_sequence_id as event_bundle_sequence_id\n,event_previous_timestamp as event_previous_timestamp\n,event_server_timestamp_offset as event_server_timestamp_offset\n,event_timestamp as event_timestamp\n,ingest_timestamp as ingest_timestamp\n,event_value_in_usd as event_value_in_usd\n,app_info.app_id as app_info_app_id\n,app_info.id as app_info_package_id\n,app_info.install_source as app_info_install_source\n,app_info.version as app_info_version\n,device.vendor_id as device_id\n,device.mobile_brand_name as device_mobile_brand_name\n,device.mobile_model_name as device_mobile_model_name\n,device.manufacturer as device_manufacturer\n,device.screen_width as device_screen_width\n,device.screen_height as device_screen_height\n,device.carrier as device_carrier\n,device.network_type as device_network_type\n,device.operating_system as device_operating_system\n,device.operating_system_version as device_operating_system_version\n,device.ua_browser ,device.ua_browser_version\n,device.ua_os\n,device.ua_os_version\n,device.ua_device\n,device.ua_device_category\n,device.system_language as device_system_language\n,device.time_zone_offset_seconds as device_time_zone_offset_seconds\n,geo.continent as geo_continent\n,geo.country as geo_country\n,geo.city as geo_city\n,geo.metro as geo_metro\n,geo.region as geo_region\n,geo.sub_continent as geo_sub_continent\n,geo.locale as geo_locale\n,platform as platform\n,project_id as project_id\n,traffic_source.name as traffic_source_name\n,traffic_source.medium as traffic_source_medium\n,traffic_source.source as traffic_source_source\n,user_first_touch_timestamp as user_first_touch_timestamp\n,user_id as user_id\n,user_pseudo_id\nfrom {{database}}.{{eventTable}}\nwhere partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\n</code></pre>"},{"location":"dashboard/activity/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>event_id</code> Dimension A SDK-generated unique id for the event user triggered when using your websites and apps Query from analytics engine <code>event_name</code> Dimension The name of the event Query from analytics engine <code>platform</code> Dimension The platform user used during the session Query from analytics engine <code>Event User Type</code> Dimension The type of user performed the event, i.e., new user or existing user Calculated field in QuickSight <code>event_date</code> Metric The date when the event was logged (YYYYMMDD format in UTC). Query from analytics engine <code>event_timestamp</code> Dimension The time (in microseconds, UTC) when the event was logged on the client. Query from analytics engine <code>app_info_version</code> Dimension The version of the app or website when event was logged Query from analytics engine <code>event_parameter_key</code> Dimension The key of the event parameter Query from analytics engine <code>event_parameter_key</code> Dimension The value of the event parameter Query from analytics engine <code>User activity number in last 7 days</code> Metrics Number of events logged in last 7 days Calculated field in QuickSight <code>User activity number in last 30 days</code> Metrics Number of events logged in last 30 days Calculated field in QuickSight"},{"location":"dashboard/activity/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"dashboard/custom-analysis/","title":"Custom report","text":"<p>One of the key benefits of this solution is that you have 100% control over the clickstream data collected from your apps and websites. You have complete flexibility to analyze the data for your specific business needs.  This article illustrates the steps of creating a custom report with an example of creating funnel analysis by using Redshift Serverless as analytics engine and QuickSight as reporting tools.</p>"},{"location":"dashboard/custom-analysis/#steps","title":"Steps","text":"<p>Creating a custom report mainly consists of two parts, the first part is to prepare the dataset in your analytics engine, the second part is to create visualization in QuickSight.</p>"},{"location":"dashboard/custom-analysis/#part-1-dataset-preparation","title":"Part 1 - Dataset preparation","text":"<ol> <li>Open Redshift Serverless dashboard</li> <li>Click the workgroup starting with <code>clickstream-&lt;project-id&gt;</code> created by the solution.</li> <li>Click on the <code>Query data</code> button, you will be directed to the Redshift Query Editor.</li> <li> <p>In the <code>Editor</code> view on the Redshift Query Editor, right click on the workgroup with name of <code>clickstream-&lt;project-id&gt;</code>. In the prompted drop-down, select <code>Edit connection</code>, you will be asked to provide connection parameters. Follow this guide to use an appropriate method to connect.</p> <p>Important</p> <p>You will need read and write permissions for the database (with name as <code>&lt;project-id&gt;</code>) to create custom view or table. For example, you can use Admin user to connect to the cluster or workgroup. If you don't know the password for the Admin user, you can reset the admin password in the Redshift Console (Learn more). </p> </li> <li> <p>If it is the first time you access the query editor, you will be prompted to configure the account, please click Config account button to open query editor.</p> </li> <li>Add a new SQL editor, and make sure you selected the correct workgroup and schema.</li> <li> <p>Create a new view for funnel analysis. In this example, we used below SQL.</p> SQL Commands <pre><code>CREATE OR REPLACE VIEW notepad.clickstream_funnel_view as\nSELECT\nplatform,\nCOUNT(DISTINCT step1_id) AS login_users,\nCOUNT(DISTINCT step2_id) AS add_button_click_users,\nCOUNT(DISTINCT step3_id) AS note_create_users\nFROM (\nSELECT\nplatform,\nuser_pseudo_id AS step1_id,\nevent_timestamp AS step1_timestamp, step2_id,\nstep2_timestamp,\nstep3_id,\nstep3_timestamp\nFROM\nnotepad.ods_events\nLEFT JOIN (\nSELECT\nuser_pseudo_id AS step2_id,\nevent_timestamp AS step2_timestamp\nFROM\nnotepad.ods_events\nWHERE\nevent_name = 'add_button_click' )\nON\nuser_pseudo_id = step2_id\nAND event_timestamp &lt; step2_timestamp\nLEFT JOIN (\nSELECT\nuser_pseudo_id AS step3_id,\nevent_timestamp AS step3_timestamp\nFROM\nnotepad.ods_events\nWHERE\nevent_name= 'note_create' )\nON\nstep3_id  = step2_id\nAND step2_timestamp &lt; step3_timestamp\nWHERE\nevent_name = 'user_login' )\ngroup by\nplatform\n</code></pre> </li> <li> <p>Go to QuickSight console, click 'Dataset', and then click 'New dataset'.</p> </li> <li> <p>In the New Dataset page, click Redshift Manual connect to add dataset, fill in the prompted form with the following parameters. </p> <ul> <li>Data source name: <code>clickstream-funnel-view-&lt;project-id&gt;</code></li> <li>Connection type: select <code>VPC connections</code> / <code>VPC Connection for Clickstream pipeline &lt;project-id&gt;</code></li> <li>Database server: input the endpoint url of the serverless workgroup, which you can find on the workgroup console.</li> <li>Port: <code>5439</code></li> <li>Database name: <code>&lt;project-id&gt;</code></li> <li>User name: name of the user you used to created the custom view in previous steps</li> <li>Password: password of the user you used to created the custom view in previous steps</li> </ul> </li> <li>Validated the connection, if ok, click Create data source button.</li> <li> <p>Choose the view from Redshift as data source - \"clickstream_funnel_view\", then</p> <ul> <li>Schema: select <code>notepad</code> </li> <li>Tables: <code>clickstream_funnel_view</code></li> </ul> <p>Tip</p> <p>You will be prompt to select <code>Import to SPICE</code> or <code>Directly query your data</code>, please select <code>Directly query your data</code>.</p> <ul> <li>Click Edit/Preview data to preview the data, once you're familiar with the data, click  PUBLISH &amp; VISUALIZE at the top-right.</li> </ul> </li> </ol>"},{"location":"dashboard/custom-analysis/#part-2-create-visulizations-in-quicksight","title":"Part 2 - Create visulizations in QuickSight","text":"<ol> <li>You will be prompt to select a layout for your visualization, select one per your need.</li> <li>Click \"+Add\" at the top-left of the screen then click \"Add visual\" button.</li> <li>Select a Visual type at the bottom-left of the screen, in this example, select Vertical bar chart</li> <li>In the Field wells, select <code>platform</code> as X axis, <code>login_user</code>, <code>add_button_click_users</code>, and <code>note_create_users</code> as Value.</li> <li>You now can publish this analysis as dashboard or continue to format it. Learn more about QuickSight visualization in this link</li> </ol>"},{"location":"dashboard/device/","title":"Device report","text":"<p>You can use the Device report to get insights into the devices that users used when using your apps or websites. The report provides more information for your user profile.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/device/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Device</code>.</li> </ol>"},{"location":"dashboard/device/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Device report are created based on the following QuickSight dataset:</p> <ul> <li><code>device_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_device_view</code> view in analytics engines (i.e., Redshift or Athena). </li> </ul> <p>Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-device-view.sql<pre><code>select\ndevice.vendor_id::varchar as device_id\n, event_date , device.mobile_brand_name::varchar\n, device.mobile_model_name::varchar\n, device.manufacturer::varchar\n, device.screen_width::int\n, device.screen_height::int\n, device.carrier::varchar\n, device.network_type::varchar\n, device.operating_system::varchar\n, device.operating_system_version::varchar\n, device.ua_browser::varchar\n, device.ua_browser_version::varchar\n, device.ua_os::varchar\n, device.ua_os_version::varchar\n, device.ua_device::varchar\n, device.ua_device_category::varchar\n, device.system_language::varchar\n, device.time_zone_offset_seconds::int\n, device.advertising_id::varchar\n, user_pseudo_id\n, user_id\n, count(event_id) as usage_num\n--pleaes update the following schema name with your schema name\nfrom {{schema}}.ods_events group by\ndevice_id\n, event_date\n, device.mobile_brand_name\n, device.mobile_model_name\n, device.manufacturer\n, device.screen_width\n, device.screen_height\n, device.carrier\n, device.network_type\n, device.operating_system\n, device.operating_system_version\n, device.ua_browser\n, device.ua_browser_version\n, device.ua_os , device.ua_os_version\n, device.ua_device\n, device.ua_device_category\n, device.system_language\n, device.time_zone_offset_seconds\n, device.advertising_id\n, user_pseudo_id\n, user_id;\n</code></pre> clickstream-device-query.sql<pre><code>select\ndevice.vendor_id as device_id\n,event_date ,device.mobile_brand_name\n,device.mobile_model_name\n,device.manufacturer\n,device.screen_width\n,device.screen_height\n,device.carrier\n,device.network_type\n,device.operating_system\n,device.operating_system_version\n,device.ua_browser\n,device.ua_browser_version\n,device.ua_os\n,device.ua_os_version\n,device.ua_device\n,device.ua_device_category\n,device.system_language\n,device.time_zone_offset_seconds\n,device.advertising_id\n,user_pseudo_id\n,user_id\n,count(event_id) as usage_num\nfrom {{database}}.{{eventTable}} where partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\ngroup by\ndevice.vendor_id\n,event_date\n,device.mobile_brand_name\n,device.mobile_model_name\n,device.manufacturer\n,device.screen_width\n,device.screen_height\n,device.carrier\n,device.network_type\n,device.operating_system\n,device.operating_system_version\n,device.ua_browser\n,device.ua_browser_version\n,device.ua_os ,device.ua_os_version\n,device.ua_device\n,device.ua_device_category\n,device.system_language\n,device.time_zone_offset_seconds\n,device.advertising_id\n,user_pseudo_id\n,user_id\n</code></pre>"},{"location":"dashboard/device/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>device_id</code> Dimension The unique ID for the device, please refer to SDK Manual for how the device id was obtained QuickSight aggregation <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>event_date</code> Dimension The event data of when the device information was logged Query from analytics engine <code>mobile_brand_name</code> Dimension The brand name for the device Query from analytics engine <code>mobile_model_name</code> Dimension The model name for the device Query from analytics engine <code>manufacturer</code> Dimension The manufacturer for the device Query from analytics engine <code>network_type</code> Dimension The network type when user logged the events Query from analytics engine <code>operating_system</code> Dimension The operating system of the device Query from analytics engine <code>operating_system_version</code> Dimension The operating system version of the device Query from analytics engine <code>screen_height</code> Dimension The screen height of the device Query from analytics engine <code>screen_width</code> Dimension The screen width of the device Query from analytics engine <code>Screen Resolution</code> Dimension The screen resolution (i.e., screen height x screen width) of the device Calculated field in QuickSight <code>system_language</code> Dimension The system language of the solution Query from analytics engine <code>us_browser</code> Dimension The browser derived from user agent Query from analytics engine <code>us_browser_version</code> Dimension The browser version derived from user agent Query from analytics engine <code>us_os</code> Dimension The operating system derived from user agent Query from analytics engine <code>us_device</code> Dimension The device derived from user agent Query from analytics engine <code>us_device_category</code> Dimension The device category derived from user agent Query from analytics engine <code>usage_num</code> Metric Number of event that logged for the device ID Query from analytics engine"},{"location":"dashboard/device/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"dashboard/engagement/","title":"Engagement report","text":"<p>You can use the Engagement report to get insights into the engagement level of the users when using your websites and apps.  This report measures user engagement by the sessions that users trigger and the web pages and app screens that users visit.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/engagement/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard.</li> <li>In the dashboard, click on the sheet with name of <code>Engagement</code>.</li> </ol>"},{"location":"dashboard/engagement/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Engagement report are created based on the QuickSight dataset of <code>session_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_session_view</code> view in analytics engines (i.e., Redshift or Athena). Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-session-view.sql<pre><code>SELECT es.session_id::varchar ,user_pseudo_id\n,platform\n,max(session_duration) as session_duration\n,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 1 else 0 end) as engaged_session\n,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 0 else 1 end) as bounced_session\n,min(session_st) as session_start_timestamp\n,sum(view) as session_views\n,sum(engagement_time) as session_engagement_time\nFROM\n(SELECT user_pseudo_id\n,event_id\n,platform\n,(select ep.value.string_value as value from {{schema}}.ods_events e, e.event_params ep where ep.key = '_session_id' and e.event_id = ods.event_id\nlimit 1\n) session_id\n,(select ep.value.int_value as value from {{schema}}.ods_events e, e.event_params ep where ep.key = '_session_duration' and e.event_id = ods.event_id\nlimit 1\n) session_duration\n,(select ep.value.int_value::bigint as value from {{schema}}.ods_events e, e.event_params ep where ep.key = '_session_start_timestamp' and e.event_id = ods.event_id\nlimit 1\n) session_st\n,(select ep.value.int_value as value from {{schema}}.ods_events e, e.event_params ep where ep.key = '_engagement_time_msec' and event_name = '_user_engagement' and e.event_id = ods.event_id\nlimit 1\n) as engagement_time\n,(case when event_name in ('_screen_view', '_page_view') then 1 else 0 end) as view\nFROM {{schema}}.ods_events ods\n) AS es\nGROUP BY 1,2,3;\n-- recompute refresh\nCREATE MATERIALIZED VIEW {{schema}}.clickstream_session_mv_2 BACKUP NO\nSORTKEY(session_id, first_sv_event_id, last_sv_event_id)\nAUTO REFRESH YES\nAS\nselect session_id, first_sv_event_id, last_sv_event_id, count(event_id) from (\nselect session_id::varchar\n, event_id\n,first_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as first_sv_event_id,\nlast_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as last_sv_event_id\nfrom (\nselect e.event_name, e.event_id, e.event_timestamp, ep.value.string_value as session_id\nfrom {{schema}}.ods_events e, e.event_params ep where e.event_name in ('_screen_view','_page_view')\nand ep.key = '_session_id') ) group by 1,2,3;\nCREATE OR REPLACE VIEW {{schema}}.clickstream_session_view AS\nwith session_f_sv_view as (\nselect * from {{schema}}.clickstream_session_mv_2 as session_f_l_sv left outer join\n(select e.event_id as event_id, ep.value.string_value as first_sv_view\nfrom {{schema}}.ods_events e, e.event_params ep\nwhere ep.key in ('_screen_name','_page_title')) t on session_f_l_sv.first_sv_event_id=t.event_id\n), session_f_l_sv_view as (\nselect * from session_f_sv_view left outer join\n(select e.event_id as event_id, ep.value.string_value as last_sv_view\nfrom {{schema}}.ods_events e, e.event_params ep\nwhere ep.key in ('_screen_name','_page_title')) t on session_f_sv_view.last_sv_event_id=t.event_id\n)\nselect CASE\nWHEN session.session_id IS NULL THEN CAST('#' AS VARCHAR)\nWHEN session.session_id = '' THEN CAST('#' AS VARCHAR)\nELSE session.session_id END AS session_id\n,user_pseudo_id\n,platform\n,session_duration::BIGINT\n,session_views::BIGINT\n,engaged_session::BIGINT\n,bounced_session\n,session_start_timestamp\n,CASE\nWHEN session.session_engagement_time IS NULL THEN CAST(0 AS BIGINT)\nELSE session.session_engagement_time END::BIGINT AS session_engagement_time\n,DATE_TRUNC('day', TIMESTAMP 'epoch' + session_start_timestamp/1000 * INTERVAL '1 second') as session_date\n,DATE_TRUNC('hour', TIMESTAMP 'epoch' + session_start_timestamp/1000 * INTERVAL '1 second') as session_date_hour\n,first_sv_view::varchar as entry_view\n,last_sv_view::varchar as exit_view\nfrom {{schema}}.clickstream_session_mv_1 as session left outer join session_f_l_sv_view on session.session_id = session_f_l_sv_view.session_id;\n</code></pre> clickstream-session-query.sql<pre><code>with base as (\nselect *\nfrom {{database}}.{{eventTable}}\nwhere partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\n),\nclickstream_session_mv_1 as (\nSELECT\nes.session_id ,user_pseudo_id\n,platform\n,max(session_duration) as session_duration\n,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 1 else 0 end) as engaged_session\n,(case when (max(session_duration)&gt;10000 or sum(view) &gt;1) then 0 else 1 end) as bounced_session\n,min(session_st) as session_start_timestamp\n,sum(view) as session_views\n,sum(engagement_time) as session_engagement_time\nFROM\n(SELECT user_pseudo_id\n,event_id\n,platform\n,(select max(ep.value.string_value) as value\nfrom base e cross join unnest(event_params) as t(ep) where ep.key = '_session_id' and e.event_id = ods.event_id) session_id\n,cast((select max(ep.value.int_value) as value\nfrom base e cross join unnest(event_params) as t(ep) where ep.key = '_session_duration' and e.event_id = ods.event_id) as integer) session_duration\n,cast((select max(ep.value.int_value) as value\nfrom base e cross join unnest(event_params) as t(ep) where ep.key = '_session_start_timestamp' and e.event_id = ods.event_id) as bigint) session_st\n,cast((select max(ep.value.int_value) as value\nfrom base e cross join unnest(event_params) as t(ep) where ep.key = '_engagement_time_msec' and event_name = '_user_engagement' and e.event_id = ods.event_id) as integer)  as engagement_time\n,cast((case when event_name in ('_screen_view', '_page_view') then 1 else 0 end) as integer) as view\nFROM base ods\n) AS es\nGROUP BY 1,2,3\n),\nclickstream_session_mv_2 as (\nselect session_id, first_sv_event_id, last_sv_event_id, count(event_id) from (\nselect session_id\n, event_id\n,first_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as first_sv_event_id,\nlast_value(event_id) over(partition by session_id order by event_timestamp asc rows between unbounded preceding and unbounded following) as last_sv_event_id\nfrom (\nselect e.event_name, e.event_id, e.event_timestamp, ep.value.string_value as session_id\nfrom base e cross join unnest(event_params) as t(ep) where e.event_name in ('_screen_view','_page_view')\nand ep.key = '_session_id') ) group by 1,2,3\n),\nsession_f_sv_view as (\nselect * from clickstream_session_mv_2 as session_f_l_sv left outer join\n(select e.event_id as event_id, ep.value.string_value as first_sv_view\nfrom base e cross join unnest(event_params) as t(ep) where ep.key in ('_screen_name','_page_title')) t on session_f_l_sv.first_sv_event_id=t.event_id\n), session_f_l_sv_view as (\nselect * from session_f_sv_view left outer join\n(select e.event_id as event_id, ep.value.string_value as last_sv_view\nfrom base e cross join unnest(event_params) as t(ep) where ep.key in ('_screen_name','_page_title')) t on session_f_sv_view.last_sv_event_id=t.event_id\n)\nselect CASE\nWHEN session.session_id IS NULL THEN CAST('#' AS VARCHAR)\nWHEN session.session_id = '' THEN CAST('#' AS VARCHAR)\nELSE session.session_id END AS session_id\n,user_pseudo_id\n,platform\n,cast(session_duration as bigint) as session_duration\n,cast(session_views as bigint) as session_duration\n,engaged_session\n,bounced_session\n,session_start_timestamp\n,session_engagement_time\n,CASE\nWHEN session.session_engagement_time IS NULL THEN CAST(0 AS BIGINT)\nELSE session.session_engagement_time END AS session_engagement_time\n,DATE_TRUNC('day', from_unixtime(session_start_timestamp/1000)) as session_date\n,DATE_TRUNC('hour', from_unixtime(session_start_timestamp/1000)) as session_date_hour\n,first_sv_view as entry_view\n,last_sv_view as exit_view\nfrom clickstream_session_mv_1 as session left outer join session_f_l_sv_view on session.session_id = session_f_l_sv_view.session_id\n</code></pre>"},{"location":"dashboard/engagement/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>session_id</code> Dimension A SDK-generated unique id for the session user triggered when using your websites and apps Query from analytics engine <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>platform</code> Dimension The platform user used during the session Query from analytics engine <code>session_duration</code> Dimension The length of the session in millisecond Query from analytics engine <code>session_views</code> Metric Number of screen view or page view within the session Query from analytics engine <code>engaged_session</code> Dimension Whether the session is engaged or not. <code>Engaged session is defined as if the session last more than 10 seconds or have two or more screen views page views</code> Query from analytics engine <code>session_start_timestamp</code> Dimension The start timestamp of the session Query from analytics engine <code>session_engagement_time</code> Dimension The total engagement time of the session in millisecond Query from analytics engine <code>entry_view</code> Dimension The screen name or page title of the first screen or page user viewed in the session Query from analytics engine <code>exit_view</code> Dimension The screen name or page title of the last screen or page user viewed in the session Query from analytics engine <code>Average engaged session per user</code> Metric Average number of session per user in the selected time period Calculated field in QuickSight <code>Average engagement time per session</code> Metric Average engagement time per session in the selected time period Calculated field in QuickSight <code>Average engagement time per user</code> Metric Average engagement time per user in the selected time period Calculated field in QuickSight <code>Average screen view per user</code> Metric Average number of screen views per user in the selected time period Calculated field in QuickSight"},{"location":"dashboard/engagement/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"dashboard/path/","title":"Path Explorer report","text":"<p>You can use the Path Explorer report to get insights into the user journey when users using your apps or websites, it helps you understand the sequence of events and screen or page transition in your apps.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/path/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Path explorer</code>.</li> </ol>"},{"location":"dashboard/path/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Device report are created based on the following QuickSight dataset:</p> <ul> <li><code>path_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_path_view</code> view in analytics engines (i.e., Redshift or Athena).</li> </ul> <p>Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena clickstream-path-view.sql<pre><code>with event_data as (\nselect user_pseudo_id\n,event_date\n,event_id\n,event_name\n,event_timestamp\n,platform\n,(select ep.value.string_value as value from {{schema}}.ods_events e, e.event_params ep where ep.key = '_session_id' and e.event_id = ods.event_id)::varchar session_id\n,(select ep.value.string_value::varchar as screen_name from {{schema}}.ods_events e, e.event_params ep where ep.key = '_screen_name' and event_name = '_screen_view' and e.event_id = ods.event_id) as current_screen\nfrom {{schema}}.ods_events ods), ranked_events as ( select *,\nDENSE_RANK() OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) event_rank,\nLAG(event_name,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) previous_event,\nLEAD(event_name,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) next_event,\nLAG(current_screen,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) previous_screen,\nLEAD(current_screen,1) OVER (PARTITION BY  user_pseudo_id, session_id ORDER BY event_timestamp ASC)  next_screen\nFROM event_data) select * from ranked_events\n</code></pre> clickstream-path-query.sql<pre><code>with base as (\nselect *\nfrom {{database}}.{{eventTable}}\nwhere partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\n),\nevent_data as (\nselect user_pseudo_id\n,event_date\n,event_id\n,event_name\n,event_timestamp\n,platform\n,(select ep.value.string_value as value from base cross join unnest(event_params) as t(ep) where ep.key = '_session_id' and event_id = ods.event_id\n) session_id\n,(select ep.value.string_value as screen_name from base cross join unnest(event_params) as t(ep) where ep.key = '_screen_name' and event_name = '_screen_view' and event_id = ods.event_id\n) as current_screen\nfrom base ods ), ranked_events as (\nselect *,\nDENSE_RANK() OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) event_rank,\nLAG(event_name,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) previous_event,\nLEAD(event_name,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) next_event,\nLAG(current_screen,1) OVER (PARTITION BY user_pseudo_id, session_id ORDER BY event_timestamp ASC) previous_screen,\nLEAD(current_screen,1) OVER (PARTITION BY  user_pseudo_id, session_id ORDER BY event_timestamp ASC)  next_screen\nFROM event_data\n) select * from ranked_events\n</code></pre>"},{"location":"dashboard/path/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>event_id</code> Dimension The unique ID for the event Query from analytics engine <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>event_date</code> Dimension The event data of when the device information was logged Query from analytics engine <code>event_timestamp</code> Dimension The timestamp when event happened Query from analytics engine <code>platform</code> Dimension The platform user used when event is logged Query from analytics engine <code>session_id</code> Dimension A SDK-generated unique id for the session user triggered when using your websites and apps Query from analytics engine <code>current_screen</code> Dimension The screen user is on for the event, 'null' for those events are not viewing screen or webpage Query from analytics engine <code>event_rank</code> Dimension The sequence of the event in a session Query from analytics engine <code>previous_event</code> Dimension The  event name of  previous event Query from analytics engine <code>next_event</code> Dimension The  event name of next event Query from analytics engine <code>previous_screen</code> Dimension The screen name of  previous screen Query from analytics engine <code>next_screen</code> Dimension The  screen name of  next screen Query from analytics engine"},{"location":"dashboard/path/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"dashboard/retention/","title":"Retention report","text":"<p>You can use the Retention report to get insights into how frequently and for how long users engage with your website or mobile app after their first visit. The report helps you understand how well your app is doing in terms of attracting users back after their first visit.</p> <p>Note: This article describes the default report. You can customize the report by applying filters or comparisons or by changing the dimensions, metrics, or charts in QuickSight. Learn more</p>"},{"location":"dashboard/retention/#view-the-report","title":"View the report","text":"<ol> <li>Access the dashboard for your application. Refer to Access dashboard</li> <li>In the dashboard, click on the sheet with name of <code>Retention</code>.</li> </ol>"},{"location":"dashboard/retention/#where-the-data-comes-from","title":"Where the data comes from","text":"<p>Retention report are created based on the following QuickSight dataset:</p> <ul> <li><code>lifecycle_weekly_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_lifecycle_weekly_view</code> view in analytics engines (i.e., Redshift or Athena). </li> <li><code>lifecycle_daily_view-&lt;app id&gt;-&lt;project id&gt;</code>, which connects to the <code>clickstream_lifecycle_daily_view</code> view in analytics engines (i.e., Redshift or Athena). </li> <li><code>retention_view-&lt;app id&gt;-&lt;project id&gt;</code> that connects to the <code>clickstream_retention_view</code> view in analytics engines</li> </ul> <p>Below is the SQL command that generates the view.</p> SQL Commands RedshiftAthena <p>clickstream-lifecycle-weekly-view.sql<pre><code>  select user_pseudo_id, -- datediff(week, '1970-01-01', dateadd(ms,event_timestamp, '1970-01-01')) as time_period\nDATE_TRUNC('week', dateadd(ms,event_timestamp, '1970-01-01')) as time_period\nfrom {{schema}}.ods_events\nwhere event_name = '_session_start' group by 1,2 order by 1,2),\n-- detect if lag and lead exists\nlag_lead as (\nselect user_pseudo_id, time_period,\nlag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period),\nlead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period)\nfrom weekly_usage),\n-- caculate lag and lead size\nlag_lead_with_diffs as (\nselect user_pseudo_id, time_period, lag, lead, datediff(week,lag,time_period) lag_size,\ndatediff(week,time_period,lead) lead_size\n-- time_period-lag lag_size, \n-- lead-time_period lead_size \nfrom lag_lead),\n-- case to lifecycle stage\ncalculated as (select time_period,\ncase when lag is null then '1-NEW'\nwhen lag_size = 1 then '2-ACTIVE'\nwhen lag_size &gt; 1 then '3-RETURN'\nend as this_week_value,\ncase when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\nelse NULL\nend as next_week_churn,\ncount(distinct user_pseudo_id)\nfrom lag_lead_with_diffs\ngroup by 1,2,3)\nselect time_period, this_week_value, sum(count) from calculated group by 1,2\nunion\nselect time_period+7, '0-CHURN', -1*sum(count) from calculated where next_week_churn is not null group by 1,2;\n</code></pre> clickstream-lifecycle-dialy-view.sql<pre><code>  select user_pseudo_id, DATE_TRUNC('day', dateadd(ms,event_timestamp, '1970-01-01')) as time_period\nfrom {{schema}}.ods_events\nwhere event_name = '_session_start' group by 1,2 order by 1,2),\n-- detect if lag and lead exists\nlag_lead as (\nselect user_pseudo_id, time_period,\nlag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period),\nlead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period)\nfrom daily_usage),\n-- caculate lag and lead size\nlag_lead_with_diffs as (\nselect user_pseudo_id, time_period, lag, lead, datediff(day,lag,time_period) lag_size,\ndatediff(day,time_period,lead) lead_size\nfrom lag_lead),\n-- case to lifecycle stage\ncalculated as (select time_period,\ncase when lag is null then '1-NEW'\nwhen lag_size = 1 then '2-ACTIVE'\nwhen lag_size &gt; 1 then '3-RETURN'\nend as this_day_value,\ncase when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\nelse NULL\nend as next_day_churn,\ncount(distinct user_pseudo_id)\nfrom lag_lead_with_diffs\ngroup by 1,2,3)\nselect time_period, this_day_value, sum(count) from calculated group by 1,2\nunion\nselect time_period+1, '0-CHURN', -1*sum(count) from calculated where next_day_churn is not null group by 1,2;\n</code></pre></p> <p>clickstream-lifecycle-weekly-query.sql<pre><code>with weekly_usage as (\nselect user_pseudo_id, DATE_TRUNC('week', event_date) as time_period\nfrom {{database}}.{{eventTable}}\nwhere partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\nand event_name = '_session_start' group by 1,2 order by 1,2\n),\nlag_lead as (\nselect user_pseudo_id, time_period,\nlag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lag,\nlead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lead\nfrom weekly_usage\n),\nlag_lead_with_diffs as (\nselect user_pseudo_id, time_period, lag, lead, date_diff('week',lag,time_period) lag_size,\ndate_diff('week',time_period,lead) lead_size\nfrom lag_lead\n),\ncalculated as (\nselect time_period,\ncase when lag is null then '1-NEW'\nwhen lag_size = 1 then '2-ACTIVE'\nwhen lag_size &gt; 1 then '3-RETURN'\nend as this_week_value,\ncase when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\nelse NULL\nend as next_week_churn,\ncount(distinct user_pseudo_id) as cnt\nfrom lag_lead_with_diffs\ngroup by 1,2,3\n)\nselect time_period, this_week_value, sum(cnt) as cnt from calculated group by 1,2\nunion\nselect date_add('day', 7, time_period), '0-CHURN', -1*sum(cnt) as cnt\nfrom calculated where next_week_churn is not null group by 1,2\n</code></pre> clickstream-lifecycle-daily-query.sql<pre><code>with daily_usage as (\nselect user_pseudo_id, DATE_TRUNC('day', event_date) as time_period\nfrom {{database}}.{{eventTable}} where partition_app = ? and partition_year &gt;= ?\nand partition_month &gt;= ?\nand partition_day &gt;= ?\nand event_name = '_session_start' group by 1,2 order by 1,2\n),\nlag_lead as (\nselect user_pseudo_id, time_period,\nlag(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lag,\nlead(time_period,1) over (partition by user_pseudo_id order by user_pseudo_id, time_period) as lead\nfrom daily_usage\n),\nlag_lead_with_diffs as (\nselect user_pseudo_id, time_period, lag, lead, date_diff('day',lag,time_period) lag_size,\ndate_diff('day',time_period,lead) lead_size\nfrom lag_lead\n),\ncalculated as (\nselect time_period,\ncase when lag is null then '1-NEW'\nwhen lag_size = 1 then '2-ACTIVE'\nwhen lag_size &gt; 1 then '3-RETURN'\nend as this_day_value,\ncase when (lead_size &gt; 1 OR lead_size IS NULL) then '0-CHURN'\nelse NULL\nend as next_day_churn,\ncount(distinct user_pseudo_id) as cnt\nfrom lag_lead_with_diffs\ngroup by 1,2,3\n)\nselect time_period, this_day_value, sum(cnt) as cnt\nfrom calculated group by 1,2\nunion\nselect date_add('day', 1, time_period) as time_period, '0-CHURN', -1*sum(cnt) as cnt from calculated where next_day_churn is not null group by 1,2;\n</code></pre></p>"},{"location":"dashboard/retention/#dimensions-and-metrics","title":"Dimensions and metrics","text":"<p>The report includes the following dimensions and metrics. You can add more dimensions or metrics by creating <code>calculated field</code> in QuickSight dateset. Learn more. </p> Field Type What is it How it's populated <code>Daily Active User (DAU)</code> Metric Number of active users per date QuickSight aggregation <code>Weekly Active User (WAU)</code> Metric Number of active users in last 7 days Calculated field in QuickSight <code>Monthly Active User (MAU)</code> Metric Number of active users in last 30 days Calculated field in QuickSight <code>user_pseudo_id</code> Dimension A SDK-generated unique id for the user Query from analytics engine <code>user_id</code> Dimension The user ID set via the setUserId API in SDK Query from analytics engine <code>DAU/WAU</code> Metric DAU/WAU % for user stickiness Calculated field in QuickSight <code>WAU/MAU</code> Metric WAU/MAU % for user stickiness Calculated field in QuickSight <code>DAU/MAU</code> Metric DAU/MAU % for user stickiness Calculated field in QuickSight <code>Event User Type</code> Dimension The type of user performed the event, i.e., new user or existing user Calculated field in QuickSight <code>User first touch date</code> Metric The first date that a user use your websites or apps Calculated field in QuickSight <code>Retention rate</code> Metric Distinct active users number / Distinct active user number by User first touch date Calculated field in QuickSight <code>time_period</code> Dimension The week or day for the user lifecycle Query from analytics engine <code>this_week_value</code> Dimension The user lifecycle stage, i.e., New, Active, Return, and Churn Query from analytics engine <code>this_day_value</code> Dimension The user lifecycle stage, i.e., New, Active, Return, and Churn Query from analytics engine"},{"location":"dashboard/retention/#sample-dashboard","title":"Sample dashboard","text":"<p>Below image is a sample dashboard for your reference.</p> <p></p>"},{"location":"deployment/","title":"Overview","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target Region you want to deploy the solution:</p> <ul> <li>At least two vacant S3 buckets.</li> </ul>"},{"location":"deployment/#deployment-in-aws-regions","title":"Deployment in AWS Regions","text":"<p>Clickstream Analytics on AWS provides two ways to authenticate and log into the solution web console. For some AWS Regions where Cognito User Pool is unavailable (for example, Hong Kong), you must launch the solution with OpenID Connect.</p> <ul> <li>Launch with Cognito User Pool. (Fastest way to get started, suitable for most AWS regions)</li> <li>Launch with OpenID Connect</li> </ul> <p>For more information about supported regions, see Regional deployments.</p>"},{"location":"deployment/#deployment-in-aws-china-regions","title":"Deployment in AWS China Regions","text":"<p>AWS China Regions do not have Cognito User Pool. You must launch the solution with OpenID Connect.</p> <ul> <li>Launch with OpenID Connect</li> </ul>"},{"location":"deployment/#deployment-within-amazon-vpc","title":"Deployment within Amazon VPC","text":"<p>Clickstream Analytics on AWS supports being deployed into an Amazon VPC, allowing access to the web console without leaving your VPC network.</p> <ul> <li>Launch within VPC</li> </ul>"},{"location":"deployment/tls-note/","title":"Tls note","text":"<p>By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations.</p>"},{"location":"deployment/with-cognito/","title":"Launch with Cognito User Pool","text":"<p>Time to deploy: Approximately 15 minutes</p>"},{"location":"deployment/with-cognito/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Launch the stack</p> <p>Step 2. Launch the web console</p>"},{"location":"deployment/with-cognito/#step-1-launch-the-stack","title":"Step 1. Launch the stack","text":"<p>This AWS CloudFormation template automatically deploys the Clickstream Analytics on AWS solution on AWS.</p> <ol> <li> <p>Sign in to the AWS Management Console and select the button to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch stack Launch stack with custom domain </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li> <p>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</p> </li> <li> <p>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</p> </li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li> <p>This solution uses the following parameters:</p> Parameter Default Description Admin User Email <code>&lt;Requires input&gt;</code> Specify the email of the Administrator. This email address will receive a temporary password to access the Clickstream Analytics on AWS web console. You can create more users directly in the provisioned Cognito User Pool after launching the solution. <p>Important</p> <p> By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations. </p> </li> <li> <p>If you are launching the solution with custom domain in AWS regions, this solution uses the additional following parameters:</p> Parameter Default Description Hosted Zone ID <code>&lt;Requires input&gt;</code> Choose the public hosted zone ID of Amazon Route 53. Hosted Zone Name <code>&lt;Requires input&gt;</code> The domain name of the public hosted zone, for example, <code>example.com</code>. Record Name <code>&lt;Requires input&gt;</code> The sub name (as known as record name in R53) of the domain name of console. For example, enter <code>clickstream</code>, if you want to use custom domain <code>clickstream.example.com</code> for the console. </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure stack options page, choose Next.</p> </li> <li> <p>On the Review page, review and confirm the settings. Select the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</p> </li> <li> <p>Choose Create stack to deploy the stack.</p> </li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 15 minutes.</p>"},{"location":"deployment/with-cognito/#step-2-launch-the-web-console","title":"Step 2. Launch the web Console","text":"<p>After the stack is successfully created, this solution generates a CloudFront domain name that gives you access to the Clickstream Analytics on AWS web console. Meanwhile, an auto-generated temporary password will be sent to your email address.</p> <ol> <li> <p>Sign in to the AWS CloudFormation console.</p> </li> <li> <p>On the Stacks page, select the solution\u2019s stack.</p> </li> <li> <p>Choose the Outputs tab and record the domain name.</p> </li> <li> <p>Open the ControlPlaneURL using a web browser, and navigate to a sign-in page.</p> </li> <li> <p>Enter the Email and the temporary password.</p> <p>a. Set a new account password.</p> <p>b. (Optional) Verify your email address for account recovery.</p> </li> <li> <p>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</p> </li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"deployment/with-oidc/","title":"Launch with OpenID Connect (OIDC)","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/with-oidc/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>The Clickstream Analytics on AWS console is served via CloudFront distribution which is considered as an Internet information service. If you are deploying the solution in AWS China Regions, the domain must have a valid ICP Recordal.</p> <ul> <li>A domain. You will use this domain to access the Clickstream Analytics on AWS console. This is required for AWS China Regions, and is optional for AWS Regions.</li> <li>An SSL certificate in AWS IAM. The SSL must be associated with the given domain. Follow this guide to upload SSL certificate to IAM. This is required for AWS China Regions only.</li> </ul>"},{"location":"deployment/with-oidc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Update the callback URL of OIDC client</p> <p>Step 4. Set up DNS Resolver</p> <p>Step 5. Launch the web console</p>"},{"location":"deployment/with-oidc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use different kinds of OpenID Connect providers. This section introduces Option 1 to Option 4.</p> <ul> <li>(Option 1) Using Amazon Cognito from another region as OIDC provider.</li> <li>(Option 2) Authing, which is an example of a third-party authentication provider.</li> <li>(Option 3) Keycloak, which is a solution maintained by AWS and can serve as an authentication identity provider.</li> <li>(Option 4) ADFS, which is a service offered by Microsoft.</li> <li>(Option 5) Other third-party authentication platforms such as Auth0.</li> </ul> <p>Follow the steps below to create an OIDC client, and obtain the <code>client_id</code> and <code>issuer</code>.</p>"},{"location":"deployment/with-oidc/#option-1-using-cognito-user-pool-from-another-region","title":"(Option 1) Using Cognito User Pool from another region","text":"<p>You can leverage the Cognito User Pool in a supported AWS Region as the OIDC provider.</p> <ol> <li>Go to the Amazon Cognito console in an AWS Region.</li> <li>Set up the hosted UI with the Amazon Cognito console based on this guide. Please pay attentions to below two configurations<ul> <li>Choose Public client when selecting the App type. Make sure don't change the selection Don't generate a client secret for Client secret.</li> <li>Add Profile in OpenID Connect scopes.</li> </ul> </li> <li> <p>Enter the Callback URL and Sign out URL using your domain name for Clickstream Analytics on AWS console.      !!! info \"Note\"         If you're not using custom domain for the console, you don't know the domain name of console. You can input a fake one, for example, <code>clickstream.example.com</code>. Then update it following guidelines in Step 3.</p> </li> <li> <p>If your hosted UI is set up, you should be able to see something like below.</p> <p></p> </li> <li> <p>Save the App client ID, User pool ID and the AWS Region to a file, which will be used later.</p> <p> </p> </li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your Cognito User Pool.</p> <ul> <li>OIDCClientId: <code>App client ID</code></li> <li>OIDCProvider: <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code></li> </ul>"},{"location":"deployment/with-oidc/#option-2-authingcn-oidc-client","title":"(Option 2) Authing.cn OIDC client","text":"<ol> <li>Go to the Authing console.</li> <li>Create a user pool if you don't have one.</li> <li>Select the user pool.</li> <li>On the left navigation bar, select Self-built App under Applications.</li> <li>Click the Create button.</li> <li>Enter the Application Name, and Subdomain.</li> <li> <p>Save the <code>App ID</code> (that is, <code>client_id</code>) and <code>Issuer</code> to a text file from Endpoint Information, which will be used later.</p> <p></p> </li> <li> <p>Update the <code>Login Callback URL</code> and <code>Logout Callback URL</code> to your IPC recorded domain name.</p> </li> <li> <p>Set the Authorization Configuration.</p> <p></p> </li> </ol> <p>You have successfully created an authing self-built application.</p> <p>In Step 2. Launch the stack, enter the parameters below from your Authing user pool.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: <code>Issuer</code></li> </ul>"},{"location":"deployment/with-oidc/#option-3-keycloak-oidc-client","title":"(Option 3) Keycloak OIDC client","text":"<ol> <li> <p>Deploy the Keycloak solution in AWS China Regions following this guide.</p> </li> <li> <p>Sign in to the Keycloak console.</p> </li> <li> <p>On the left navigation bar, select Add realm. Skip this step if you already have a realm.</p> </li> <li> <p>Go to the realm setting page. Choose Endpoints, and then OpenID Endpoint Configuration from the list.</p> <p></p> </li> <li> <p>In the JSON file that opens up in your browser, record the issuer value which will be used later.</p> <p></p> </li> <li> <p>Go back to Keycloak console and select Clients on the left navigation bar, and choose Create.</p> </li> <li>Enter a Client ID, which must contain letters (case-insensitive) or numbers. Record the Client ID which will be used later.</li> <li> <p>Change client settings. Enter <code>http[s]://&lt;Clickstream Analytics on AWS Console domain&gt;/signin</code> in Valid Redirect URIs\uff0cand enter <code>&lt;console domain&gt;</code> and <code>+</code> in Web Origins.</p> <p>Tip</p> <p>If you're not using custom domain for the console, the domain name of console is not available yet. You can enter a fake one, for example, <code>clickstream.example.com</code>, and then update it following guidelines in Step 3.</p> </li> <li> <p>In the Advanced Settings, set the Access Token Lifespan to at least 5 minutes.</p> </li> <li>Select Users on the left navigation bar.</li> <li>Click Add user and enter Username.</li> <li>After the user is created, select Credentials, and enter Password.</li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your Keycloak realm.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: <code>https://&lt;KEYCLOAK_DOMAIN_NAME&gt;/auth/realms/&lt;REALM_NAME&gt;</code></li> </ul>"},{"location":"deployment/with-oidc/#option-4-adfs-openid-connect-client","title":"(Option 4) ADFS OpenID Connect Client","text":"<ol> <li>Make sure your ADFS is installed. For information about how to install ADFS, refer to this guide.</li> <li>Make sure you can log in to the ADFS Sign On page. The URL should be <code>https://adfs.domain.com/adfs/ls/idpinitiatedSignOn.aspx</code>, and you need to replace adfs.domain.com with your real ADFS domain.</li> <li>Log on your Domain Controller, and open Active Directory Users and Computers.</li> <li> <p>Create a Security Group for Clickstream Analytics on AWS Users, and add your planned Clickstream Analytics on AWS users to this Security Group.</p> </li> <li> <p>Log on to ADFS server, and open ADFS Management.</p> </li> <li> <p>Right click Application Groups, choose Application Group, and enter the name for the Application Group. Select Web browser accessing a web application option under Client-Server Applications, and choose Next.</p> </li> <li> <p>Record the Client Identifier (<code>client_id</code>) under Redirect URI, enter your Clickstream Analytics on AWS domain (for example, <code>xx.example.com</code>), and choose Add, and then choose Next.</p> </li> <li> <p>In the Choose Access Control Policy window, select Permit specific group, choose parameters under Policy part, add the created Security Group in Step 4, then click Next. You can configure other access control policy based on your requirements.</p> </li> <li> <p>Under Summary window, choose Next, and choose Close.</p> </li> <li> <p>Open the Windows PowerShell on ADFS Server, and run the following commands to configure ADFS to allow CORS for your planned URL.</p> <pre><code>Set-AdfsResponseHeaders -EnableCORS $true\nSet-AdfsResponseHeaders -CORSTrustedOrigins https://&lt;your-clickstream-analytics-on-aws-domain&gt;\n</code></pre> </li> <li> <p>Under Windows PowerShell on ADFS server, run the following command to get the Issuer (<code>issuer</code>) of ADFS, which is similar to <code>https://adfs.example.com/adfs</code>.</p> <pre><code>Get-ADFSProperties | Select IdTokenIssuer\n</code></pre> <p></p> </li> </ol> <p>In Step 2. Launch the stack, enter the parameters below from your ADFS server.</p> <ul> <li>OIDCClientId: <code>client id</code></li> <li>OIDCProvider: Get the server of the issuer from above step 11</li> </ul>"},{"location":"deployment/with-oidc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch in AWS Regions Launch with custom domain in AWS Regions Launch in AWS China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li> <p>This solution uses the following parameters:</p> Parameter Default Description OIDCClientId <code>&lt;Requires input&gt;</code> OpenID Connect client Id. OIDCProvider <code>&lt;Requires input&gt;</code> OpenID Connect provider issuer. The issuer must begin with <code>https://</code> <p>Important</p> <p> By default, this deployment uses TLSv1.0 and TLSv1.1 in CloudFront. However, we recommend that you manually configure CloudFront to use the more secure TLSv1.2/TLSv1.3 and apply for a certificate and custom domain to enable this. We highly recommend that you update your TLS configuration and cipher suite selection according to the following recommendations:</p> <ul> <li>Transport Layer Security Protocol: Upgrade to TLSv1.2 or higher</li> <li>Key Exchange: ECDHE</li> <li>Block Cipher Mode of Operation: GCM</li> <li>Authentication: ECDSA</li> <li>Encryption Cipher: AES256</li> <li>Message Authentication: SHA(256/384/any hash function except for SHA1)</li> </ul> <p>Such as TLSv1.2_2021 can meet the above recommendations. </p> </li> <li> <p>If you are launching the solution with custom domain in AWS Regions, this solution has the following additional parameters:</p> Parameter Default Description Hosted Zone ID <code>&lt;Requires input&gt;</code> Choose the public hosted zone ID of Amazon Route 53. Hosted Zone Name <code>&lt;Requires input&gt;</code> The domain name of the public hosted zone, for example, <code>example.com</code>. Record Name <code>&lt;Requires input&gt;</code> The sub name (as known as record name in R53) of the domain name of console. For example, enter <code>clickstream</code> if you want to use custom domain <code>clickstream.example.com</code> for the console. </li> <li> <p>If you are launching the solution in AWS China Regions, this solution has the following additional parameters:</p> Parameter Default Description Domain <code>&lt;Requires input&gt;</code> Custom domain for Clickstream Analytics on AWS console. Do NOT add <code>http(s)</code> prefix. IamCertificateID <code>&lt;Requires input&gt;</code> The ID of the SSL certificate in IAM. The ID is composed of 21 characters of capital letters and digits. Use the <code>list-server-certificates</code> command to retrieve the ID. </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"deployment/with-oidc/#step-3-update-the-callback-url-of-oidc-client","title":"Step 3. Update the callback URL of OIDC client","text":"<p>Important</p> <p>If you don't deploy stack with custom domain, you must complete below steps.</p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL as the endpoint.</li> <li>Update or add the callback URL to your OIDC.<ol> <li>For Cognito, add or update the url in Allowed callback URL of your client with value <code>${ControlPlaneURL}/signin</code>. NOTE: The url must start with <code>https://</code>.</li> <li>For Keycloak, add or update the url in Valid Redirect URIs of your client with value <code>${ControlPlaneURL}/signin</code>.</li> <li>For Authing.cn, add or update the url in Login Callback URL of Authentication Configuration.</li> </ol> </li> </ol>"},{"location":"deployment/with-oidc/#step-4-setup-dns-resolver","title":"Step 4. Setup DNS Resolver","text":"<p>Important</p> <p>If you deploy stack in AWS Regions, you can skip this step.</p> <p>This solution provisions a CloudFront distribution that gives you access to the Clickstream Analytics on AWS console.</p> <ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL and CloudFrontDomainName.</li> <li>Create a CNAME record for ControlPlaneURL in DNS resolver, which points to the domain CloudFrontDomainName obtained in previous step.</li> </ol>"},{"location":"deployment/with-oidc/#step-5-launch-the-web-console","title":"Step 5. Launch the web console","text":"<p>Important</p> <p>Your login credentials is managed by the OIDC provider. Before signing in to the Clickstream Analytics on AWS console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Use the previously assigned domain name or the generated ControlPlaneURL in a web browser.</li> <li>Choose Sign In, and navigate to OIDC provider.</li> <li>Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"deployment/within-vpc/","title":"Launch within VPC","text":"<p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/within-vpc/#prerequisites","title":"Prerequisites","text":"<p>Review all the considerations and make sure you have the following in the target region you want to deploy the solution:</p> <ul> <li>At least one Amazon VPC.</li> <li>At least two private (with NAT gateways or instances) subnets across two AZs.</li> </ul>"},{"location":"deployment/within-vpc/#deployment-overview","title":"Deployment Overview","text":"<p>Use the following steps to deploy this solution on AWS.</p> <p>Step 1. Create OIDC client</p> <p>Step 2. Launch the stack</p> <p>Step 3. Update the callback url of OIDC client</p> <p>Step 4. Launch the web console</p>"},{"location":"deployment/within-vpc/#step-1-create-oidc-client","title":"Step 1. Create OIDC client","text":"<p>You can use existing OpenID Connect (OIDC) provider or following this guide to create an OIDC client.</p> <p>Tip</p> <p>This solution deploys the console in VPC without requiring SSL certificate by default. You have to use an OIDC client to support callback url with <code>http</code> protocol.</p>"},{"location":"deployment/within-vpc/#step-2-launch-the-stack","title":"Step 2. Launch the stack","text":"<ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Launch in AWS Regions Launch in AWS China Regions </li> <li> <p>The template is launched in the default region after you log in to the console. To launch the Clickstream Analytics on AWS solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL shows in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a name to your solution stack. For information about naming character limitations, refer to IAM and AWS STS quotas in the AWS Identity and Access Management User Guide.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary.</p> <ul> <li>This solution uses the following parameters:</li> </ul> Parameter Default Description VpcId <code>&lt;Requires input&gt;</code> Select the VPC in which the solution will be deployed. PrivateSubnets <code>&lt;Requires input&gt;</code> Select the subnets in which the solution will be deployed. Note: You must choose two subnets across two AZs at least. OIDCClientId <code>&lt;Requires input&gt;</code> OpenID Connect client Id. OIDCProvider <code>&lt;Requires input&gt;</code> OpenID Connect provider issuer. The issuer must begin with <code>https://</code> </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Check the box acknowledging that the template creates AWS Identity and Access Management (IAM) resources.</li> <li>Choose Create stack  to deploy the stack.</li> </ol> <p>You can view the status of the stack in the AWS CloudFormation console in the Status column. You should receive a CREATE_COMPLETE status in approximately 10 minutes.</p>"},{"location":"deployment/within-vpc/#step-3-update-the-callback-url-of-oidc-client","title":"Step 3. Update the callback URL of OIDC client","text":"<ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select the solution's stack.</li> <li>Choose the Outputs tab.</li> <li>Obtain the ControlPlaneURL as the endpoint.</li> <li>Update or add the callback URL ${ControlPlaneURL}/signin to your OIDC client.<ol> <li>For Keycloak, add or update the url in Valid Redirect URIs.</li> <li>For Authing.cn, add or update the url in Login Callback URL of Authentication Configuration.</li> </ol> </li> </ol>"},{"location":"deployment/within-vpc/#step-4-launch-the-web-console","title":"Step 4. Launch the web console","text":"<p>Important</p> <p>Your login credentials is managed by the OIDC provider. Before signing in to the Clickstream Analytics on AWS console, make sure you have created at least one user in the OIDC provider's user pool.</p> <ol> <li>Because you deploy the solution console in your VPC without public access, you have to setup a network connection to the solution console serving by an internal application load balancer. There are some options for your reference.<ol> <li>(Option 1) Use bastion host, for example, Linux Bastion Hosts on AWS solution</li> <li>(Option 2) Use AWS Client VPN or AWS Site-to-Site VPN</li> <li>(Option 3) Use AWS Direct Connect</li> </ol> </li> <li>The application load balancer only allows the traffic from specified security group, you can find the security group id from the output named SourceSecurityGroup from the stack you deployed in step 2. Then attach the security group to your bastion host or other source to access the solution console.</li> <li>Use the previously assigned domain name or the generated ControlPlaneURL in a web browser.</li> <li>Choose Sign In, and navigate to OIDC provider.</li> <li>Enter sign-in credentials. You may be asked to change your default password for first-time login, which depends on your OIDC provider's policy.</li> <li>After the verification is complete, the system opens the Clickstream Analytics on AWS web console.</li> </ol> <p>Once you have logged into the Clickstream Analytics on AWS console, you can start to create a project for your applications.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>After deploying the solution, refer to this section to quickly learn how to leverage the Clickstream Analytics on AWS solution to collect and analyze event data from your applications. This chapter shows you how to create a serverless data pipeline to collect data from an Android application, and view the out-of-the-box analytics dashboards. </p>"},{"location":"getting-started/#steps","title":"Steps","text":"<ul> <li>Step 1: Create a project. Create a project to get started.</li> <li>Step 2: Configure a data pipeline. Configure a data pipeline with serverless infrastructure.</li> <li>Step 3: Integrate SDK. Integrate SDK into your application to automatically collect data and send data to the pipeline.</li> <li>Step 4: Access built-in dashboard. View the out-of-the-box dashboards based on the data automatically collected from your applications.  </li> </ul>"},{"location":"getting-started/1.create-project/","title":"Step 1 - Create a project","text":"<p>To get started with the Clickstream Analytics on AWS solution, you need to create a project in the solution console. A project is like a container that groups all the AWS resources provisioned for collecting and analyzing the clickstream data from your apps.</p>"},{"location":"getting-started/1.create-project/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have deployed the Clickstream Analytics on AWS solution. If you haven't, please refer to the deployment guide.</p>"},{"location":"getting-started/1.create-project/#steps","title":"Steps","text":"<p>Following below steps to create a project.</p> <ol> <li>Sign in to the Clickstream Analytics on AWS Console.</li> <li>On the Home page, choose Create Project.</li> <li>In the window that pops up, enter a project name, for example, <code>quickstart</code>.</li> <li>(Optional) Customize the project ID that was automatically created by solution. To do so, click the <code>edit</code> icon and update the project ID as per your need.</li> <li>Provide a description for your project, for example, <code>This is a demo project</code>.</li> <li>Choose Next.</li> <li>Provide an email address to receive notification regarding this project, for example, <code>email@example.com</code>, and choose Next.</li> <li>Specify an environment type for this project. In this example, select <code>Dev</code>.</li> <li>Choose Create. Wait until the project creation completed, and you will be directed to the Projects page.</li> </ol> <p>We have completed all the steps of creating a project.</p>"},{"location":"getting-started/1.create-project/#next","title":"Next","text":"<ul> <li>Configure data pipeline</li> </ul>"},{"location":"getting-started/2.config-pipeline/","title":"Step 2 - Configure pipeline","text":"<p>After you create a project, you need to configure the data pipeline for it. A data pipeline is a set of integrated modules that collect and process the clickstream data sent from your applications. A data pipeline contains four modules, namely data ingestion, data processing, data modeling and reporting. For more information, see pipeline management.</p> <p>Here we provide an example with steps to create a data pipeline with end-to-end serverless infrastructure.</p>"},{"location":"getting-started/2.config-pipeline/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, choose Projects, then select the project you just created in Step 1, choose View Details in the top right corner to navigate to the project homepage.</li> <li>Choose Configure pipeline, and it will bring you to the wizard of creating data pipeline for your project.</li> <li> <p>On the Basic information page, fill in the form as follows:</p> <ul> <li>AWS Region: <code>us-east-1</code></li> <li>VPC: select a VPC that meets the following requirements<ul> <li>At least two public subnets across two different AZs (Availability Zone)</li> <li>At least two private subnets across two different AZs</li> <li>One NAT Gateway or Instance</li> </ul> </li> <li>Data collection SDK: <code>Clickstream SDK</code></li> <li>Data location: select an S3 bucket. (You can create one bucket, and select it after clicking the Refresh button.)</li> </ul> <p>Tip</p> <p>If you don't have a VPC meet the criteria, you can create a VPC by using VPC creation wizard quickly. For more information, see Create a VPC.</p> </li> <li> <p>Choose Next.</p> </li> <li> <p>On the Configure ingestion page, fill in the information as follows:</p> <ol> <li>Fill in the form of Ingestion endpoint settings.<ul> <li>Public Subnets: Select two public subnets in two different AZs</li> <li>Private Subnets: Select two private subnets in the same AZs as public subnets</li> <li>Ingestion capacity: Keep the default values</li> <li>Enable HTTPS: Uncheck and then Acknowledge the security warning</li> <li>Additional settings: Keep the default values</li> </ul> </li> <li>Fill in the form of Data sink settings.<ul> <li>Sink type: <code>Amazon Kinesis Data Stream(KDS)</code></li> <li>Provision mode: <code>On-demand</code></li> <li>In Additional Settings, change Sink Maximum Interval to <code>60</code> and Batch Size to <code>1000</code></li> </ul> </li> <li>Click Next to move to step 3.</li> </ol> <p>Important</p> <p>Using HTTP is not a recommended configuration for production workload. This example configuration is to help you get started quicker.</p> </li> <li> <p>On the Configure data processing information, fill in the information as follows:</p> <ul> <li>In the form of Enable data processing, toggle on the Enable data processing</li> <li>In the form of Execution parameters,<ul> <li>Data processing interval:<ul> <li>Select <code>Fixed Rate</code></li> <li>Enter <code>10</code></li> <li>Select <code>Minutes</code></li> </ul> </li> <li>Event freshness: <code>35</code> <code>Days</code></li> </ul> </li> <li>In the form of Enrichment plugins, make sure the two plugins of IP lookup and UA parser are selected.</li> <li>In the form of Analytics engine, fill in the form as follow:<ul> <li>Check the box for Redshift</li> <li>Select the Redshift Serverless</li> <li>Keep Base RPU as 8</li> <li>VPC: select the default VPC or the same one you selected previously in the last step</li> <li>Security group: select the <code>default</code> security group</li> <li>Subnet: select three subnets across three different AZs</li> <li>Keep Athena selection as default</li> </ul> </li> <li>Choose Next.</li> </ul> </li> <li> <p>On the Reporting page, fill in the form as follows:</p> <ul> <li>If your AWS account has not subscribed to QuickSight, please follow this guide to subscribe.</li> <li>After your AWS account subscribed to QuickSight Enterprise edition, choose one of QuickSight users for the solution to use to create out-of-the-box dashboards.</li> <li>Choose Next.</li> </ul> </li> <li> <p>On the Review and launch page, review your pipeline configuration details. If everything is configured properly, choose Create.</p> </li> </ol> <p>We have completed all the steps of configuring a pipeline for your project. This pipeline will take about 20 minutes to create, and please wait for the pipeline status change to be Active in pipeline detail page.</p>"},{"location":"getting-started/2.config-pipeline/#next","title":"Next","text":"<ul> <li>Integrate SDK</li> </ul>"},{"location":"getting-started/3.integrate-sdk/","title":"Step 3 - Integrate SDK","text":"<p>Once pipeline's status becomes <code>Active</code>, it is ready to receive clickstream data. Now you need to register an application to the pipeline, then you can integrate SDK into your application to enable it to send data to the pipeline.</p>"},{"location":"getting-started/3.integrate-sdk/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, choose Projects, then select the project (<code>quickstart</code>) you just created in previous steps, click its title, and it will bring you to the project page.</li> <li>Choose + Add application to start adding application to the pipeline.</li> <li> <p>Fill in the form as follows:</p> <ul> <li>App name: <code>test-app</code></li> <li>App ID: The system will generate one ID based on the name, and you can customize it if needed.</li> <li>Description: <code>A test app for Clickstream Analytics on AWS solution</code></li> <li>Android package name: leave it blank</li> <li>App Bundle ID: leave it blank</li> </ul> </li> <li> <p>Choose Register App &amp; Generate SDK Instruction, and wait for the registration to be completed.</p> </li> <li> <p>Select the tab Android, and you will see the detailed instruction of adding SDK into your application. You can follow the steps to add SDK.</p> </li> <li> <p>Click the Download the config json file button to download the config file, and keep this file open, which will be used later.</p> </li> </ol> <p>It will take about 3 ~ 5 minutes to update the pipeline with the application you just add. When you see the pipeline status become Active again, it is ready to receive data from your application. </p> <p>We have completed all the steps of adding an application to a project.</p>"},{"location":"getting-started/3.integrate-sdk/#generate-sample-data","title":"Generate sample data","text":"<p>You might not have immediate access to integrate SDK with your app. In this case, we provide a Python script to generate sample data to the pipeline you just configured, so that you can view and experience the analytics dashboards.</p>"},{"location":"getting-started/3.integrate-sdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> </ul>"},{"location":"getting-started/3.integrate-sdk/#steps_1","title":"Steps","text":"<ol> <li>Clone the repository to your local environment.    <pre><code>git clone https://github.com/awslabs/clickstream-analytics-on-aws.git\n</code></pre></li> <li> <p>After you cloned the repository, change directory into the <code>examples/standalone-data-generator</code> project folder.</p> </li> <li> <p>Install the dependencies of the project.     <pre><code>pip3 install requests\n</code></pre></p> </li> <li> <p>Put <code>amplifyconfiguration.json</code> into the root of <code>examples/standalone-data-generator</code> which you downloaded in register an app step. See the <code>examples/standalone-data-generator/README.md</code> for more usages.</p> </li> <li> <p>Open an terminal at this project folder location. For example, if you are using Visual Studio Code IDE, at the top of Visual Studio Code, click Terminal -&gt; New Terminal to open a terminal.</p> </li> <li> <p>Copy the following command and paste it to the terminal:</p> <pre><code>python3 create_event.py\n</code></pre> </li> </ol> <p>Hit <code>Enter</code> key in terminal to execute the program. If you see the following output, this means that the program execution is complete.</p> <pre><code>job finished, upload 4360476 events, cost: 95100ms\n</code></pre> <p>This process will take about 10 minutes with default configuration. After job is finished, you can move to next step.</p>"},{"location":"getting-started/3.integrate-sdk/#next","title":"Next","text":"<ul> <li>View dashboard</li> </ul>"},{"location":"getting-started/4.view-dashboard/","title":"Step 4 - View dashboard","text":"<p>After your application sends data (or the sample data are sent) to the pipeline, you can view the out-of-the-box dashboards that the solution created in QuickSight.</p>"},{"location":"getting-started/4.view-dashboard/#steps","title":"Steps","text":"<ol> <li>Log into Clickstream Analytics on AWS Console.</li> <li>In the left navigation pane, choose Projects, then select the project (<code>quickstart</code>) you just created in previous steps, click its title, and it will bring you to the project page.</li> <li>On the project detail page, click <code>pipeline ID</code> or View Details button, and it will bring you to the pipeline detail page.</li> <li>In the pipeline details page, select the Reporting tab, and you will see the link to the dashboards created for your app.</li> <li>Click the dashboard link with the name of your app, and it will bring you to the QuickSight dashboard. You should see a dashboard similar to below in your QuickSight Account.     </li> <li>Click the dashboard with name starting with <code>Clickstream</code>.</li> </ol> <p>Congratulations! You have completed the getting started tutorial. You can explore the dashboards or continue to learn more about this solution later.</p>"},{"location":"getting-started/4.view-dashboard/#next","title":"Next","text":"<ul> <li>Pipeline management</li> </ul>"},{"location":"pipeline-mgmt/","title":"Data pipeline","text":"<p>Data pipeline is the core functionality of this solution. In the Clickstream Analytics on AWS solution, we define a data pipeline as a sequence of integrated AWS services that ingest, process, and model the clickstream data into a destination data warehouse for analytics and visualization. It is also designed to efficiently and reliably collect data from your websites and apps to a S3-based data lake, where it can be further processed, analyzed, and utilized for additional use cases (such as real-time monitoring, and recommendation).</p>"},{"location":"pipeline-mgmt/#concepts","title":"Concepts","text":"<p>Before creating a data pipeline, you can learn a few concepts in this solution so that you can configure the data pipeline to best fit your business goal.</p>"},{"location":"pipeline-mgmt/#project","title":"Project","text":"<p>A project in this solution is the top-level entity, like a container, that groups your apps and data pipeline for collecting and processing clickstream data. One project contains one data pipeline, and can have one or more apps registered to it.</p>"},{"location":"pipeline-mgmt/#data-pipeline_1","title":"Data pipeline","text":"<p>A data pipeline is deployed into one AWS region, which means all the underlining resources are created in one AWS region. A data pipeline in this solution contains four modules:</p> <ul> <li>Data ingestion: a web service that provides an endpoint to collect data through HTTP requests, and sink the data in streaming services or S3.</li> <li>Data processing: a module that transforms raw data to the solution schema and enriches data with additional dimensions.</li> <li>Data modeling: a module that aggregates data to calculate metrics for business analytics.</li> <li>Reporting: a module that creates metrics and out-of-the-box visualizations in QuickSight.</li> </ul>"},{"location":"pipeline-mgmt/#app","title":"App","text":"<p>An app in this solution can represent an application in your business, which might be built on one or multiple platforms (for example, Android, iOS, and Web).</p>"},{"location":"pipeline-mgmt/#dashboard","title":"Dashboard","text":"<p>For each app registered in the dashboard, the solution will create a QuickSight dashboard for each App in the AWS region as pipeline.</p> <p>Below is a diagram to help you better understand those concepts and their relationship with each other in the AWS context.</p> <p></p>"},{"location":"pipeline-mgmt/#prerequisites","title":"Prerequisites","text":"<p>You can configure the pipeline in all AWS regions. For opt-in regions, you need to enable them firstly.</p> <p>Before you start to configure the pipeline in a specific region, make sure you have the following in the target region:</p> <ul> <li>At least one Amazon VPC.</li> <li>At least two public subnets across two AZs in the VPC.</li> <li>At least two private (with NAT gateways or instances) subnets across two AZs, or at least two isolated subnets across two AZs in the VPC. If you want to deploy the solution resources in the isolated subnets, you have to create VPC endpoints for below AWS services,<ul> <li><code>s3</code>, <code>logs</code>, <code>ecr.api</code>, <code>ecr.dkr</code>, <code>ecs</code>, <code>ecs-agent</code>, <code>ecs-telemetry</code>.</li> <li><code>kinesis-streams</code> if you use KDS as sink buffer in ingestion module.</li> <li><code>emr-serverless</code>, <code>glue</code> if you enable data processing module.</li> <li><code>redshift-data</code>, <code>sts</code>, <code>dynamodb</code>, <code>states</code> and <code>lambda</code> if you enable Redshift as analytics engine in data modeling module.</li> </ul> </li> <li>an Amazon S3 bucket located in the same Region.</li> <li>If you need to enable Redshift Serverless as analytics engine in data modeling module, you need have subnets across at least three AZs.</li> <li>QuickSight Enterprise edition subscription is required if the reporting is enable.</li> </ul>"},{"location":"pipeline-mgmt/pipe-mgmt/","title":"Data pipeline management","text":"<p>This solution provides three features to help you manage and operate the data pipeline after it gets created.</p>"},{"location":"pipeline-mgmt/pipe-mgmt/#monitoring-and-alarms","title":"Monitoring and Alarms","text":"<p>The solution collects metrics from each resource in the data pipeline and creates monitoring dashboards in CloudWatch, which provides you a comprehensive view into the pipeline status. It also provides a set of alarms that will notify project owner if anything goes abnormal. </p> <p>Following are steps to view monitoring dashboards and alarms.</p>"},{"location":"pipeline-mgmt/pipe-mgmt/#monitoring-dashboards","title":"Monitoring dashboards","text":"<p>To view monitoring dashboard for a data pipeline, follows below steps:</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>Click the tab of \"Monitoring\"</li> <li>In the tab, click on the View in CloudWatch button, which will direct you to the monitoring dashboard.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#alarms","title":"Alarms","text":"<p>To view alarms for a data pipeline, follows below steps:</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>Click the tab of \"Alarms\"</li> <li>In the tab, you can view all the alarms. You can also click on the View in CloudWatch button, which will direct you to CloudWatch alarm pages to view alarm details.</li> <li>You can also enable or disable an alarm by select the alarm then click on the Enable or Disable buttons.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#pipeline-modification","title":"Pipeline modification","text":"<p>You are able to modify some configuration the data pipeline after it created, follow below steps to update a pipeline.</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>In the project details page, click on the Edit button, which will bring you to the pipeline creation wizard page.</li> <li>You will notice some configuration are in disable mode, which means they cannot be updated after creation.</li> <li>For those configuration options are editable, you can update them.</li> <li>After you edit the configuration, click Next until you reach last page, click Save.</li> <li>You will see pipeline is in <code>Updating</code> status.</li> </ol>"},{"location":"pipeline-mgmt/pipe-mgmt/#pipeline-upgrade","title":"Pipeline upgrade","text":"<p>As we continue enhance the solution, there will new version of pipeline available for you use after you update the solution control panel.</p> <ol> <li>Go to project detail page</li> <li>Click on <code>project id</code> or View Details button, which will direct to the pipeline detail page.</li> <li>In the project details page, click on the Upgrade button</li> <li>You will be prompted to confirm the upgrade action.</li> <li>Click on Confirm, the pipeline will be in <code>Upgrading</code> status.</li> </ol>"},{"location":"pipeline-mgmt/data-modeling/configure-data-modeling/","title":"Data modeling settings","text":"<p>Once the data pipeline processes the event data, you can load the data into an analytics engine for data modeling, such as Redshift or Athena, where data will be aggregated and organized into different views (such as event, device, session), as well as calculated metrics that are commonly used. Below are the preset data views this solution provides if you choose to enable data modeling module.</p>"},{"location":"pipeline-mgmt/data-modeling/configure-data-modeling/#preset-data-views","title":"Preset data views","text":"Data model name Redshift Athena Description clickstream_ods_event_view Materialized view View A view contains all event dimensions clickstream_ods_event_attr_view Materialized view View A view contains all event parameters. clickstream_user_dim_view Materialized view View A view contains all user dimensions. clickstream_user_attr_view Materialized view View A view contains all user custom attributes. clickstream_session_view Materialized view View A view contains all session dimension and relevant metrics, e.g.,session duration, session views. clickstream_retention_view Materialized view View A view contains metrics of retentions by dates and return days. clickstream_lifecycle_daily_view Materialized view View A view contains metrics of user number by lifecycle stages by day, i.e., New, Active, Return, Churn. clickstream_lifecycle_weekly_view Materialized view View A view contains metrics of user number by lifecycle stages by week, i.e., New, Active, Return, Churn. clickstream_path_view Materialized view View A view contains information about user journey within each session. <p>You can choose to use Redshift or Athena, or both. </p> <p>Tip</p> <p>We recommended you select both, that is, using Redshift for hot data modeling and using Athena for all-time data analysis.</p> <p>You can set below configurations for Redshift.  </p> <ul> <li> <p>Redshift Mode: Select Redshift serverless or provisioned mode.</p> <ul> <li> <p>Serverless mode</p> <ul> <li> <p>Base RPU: RPU stands for Redshift Processing Unit. Amazon Redshift Serverless measures data warehouse capacity in RPUs, which are resources used to handle workloads. The base capacity specifies the base data warehouse capacity Amazon Redshift uses to serve queries and is specified in RPUs. Setting higher base capacity improves query performance, especially for data processing jobs that consume a lot of resources.</p> </li> <li> <p>VPC: A virtual private cloud (VPC) based on the Amazon VPC service is your private, logically isolated network in the AWS Cloud.</p> <p>Note: If you place the cluster within the isolated subnets, the VPC must have VPC endpoints for S3, Logs, Dynamodb, STS, States, Redshift and Redshift-data service.</p> </li> <li> <p>Security Group: This VPC security group defines which subnets and IP ranges can access the endpoint of Redshift cluster.</p> </li> <li> <p>Subnets: Select at least three existing VPC subnets.</p> <p>Note: We recommend using private subnets to deploy for following security best practices.</p> </li> </ul> </li> <li> <p>Provisioned mode</p> <ul> <li> <p>Redshift Cluster: With a provisioned Amazon Redshift cluster, you build a cluster with node types that meet your cost and performance specifications. You have to set up, tune, and manage Amazon Redshift provisioned clusters.</p> </li> <li> <p>Database user: The solution needs permissions to access and create database in Redshift cluster. By default, it grants Redshift Data API with the permissions of the admin user to execute the commands to create DB, tables, and views, as well as loading data.</p> </li> </ul> </li> <li> <p>Data range: Considering the cost performance issue of having Redshift to save all the data, we recommend that Redshift save hot data and that all data are stored in S3. It is necessary to delete expired data in Redshift on a regular basis.</p> </li> </ul> </li> <li> <p>Additional Settings</p> <ul> <li>User table upsert frequency: Since all versions of user properties are saved in Redshift. We create a user-scoped custom dimension table <code>dim_users</code> in DWD layer so the BI dashboard can report on the latest user property. The workflow run on schedule to upsert (update and insert) users.</li> </ul> </li> <li> <p>Athena: Choose Athena to query all data on S3 using the table created in the Glue Data Catalog.</p> </li> </ul>"},{"location":"pipeline-mgmt/data-processing/","title":"Data Processing","text":"<p>Clickstream Analytics on AWS provides an inbuilt data schema to parse and model the raw event data sent from your web and mobile apps, which makes it easy for you to analyze the data in analytics engines (such as Redshift and Athena). </p> <p>Data Processing module includes two functionalities:</p> <ul> <li>Transformation: Extract the data from files sank by ingestion module, then parse each event data and transform them to solution data model.</li> <li>Enrichment: Add additional dimensions/fields to event data.</li> </ul> <p>This chapter includes:</p> <ul> <li>Data schema</li> <li>Configure execution parameters</li> <li>Configure ETL custom plugins</li> </ul>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/","title":"Configure Execution Parameters","text":"<p>Execution parameters control how the transformation and enrichment jobs are orchestrated.</p>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#parameters","title":"Parameters","text":"<p>You can configure the following Execution parameters after you toggle on Enable data processing.</p> Parameter Description Values Data processing interval/Fixed Rate Specify the interval to batch the data for ETL processing by fixed rate 1 hour 12 hours1 day Data processing interval/Cron Expression Specify the interval to batch the data for ETL processing by cron expression <code>cron(0 * * ? *)</code> <code>cron(0 0,12 * ? *)</code><code>cron(0 0 * ? *)</code> Event freshness Specify the days after which the solution will ignore the event data. For example, if you specify 3 days for this parameter, the solution will ignore any event which arrived more than 3 days after the events are triggered 3 days 5 days 30 days"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#cron-expression-syntax","title":"Cron Expression Syntax","text":"<p>Syntax</p> <p><code>cron(minutes hours day-of-month month day-of-week year)</code></p> <p>For more information, refer to Cron-based schedules.</p>"},{"location":"pipeline-mgmt/data-processing/configure-execution-para/#config-spark-job-parameters","title":"Config Spark job parameters","text":"<p>By default, the EMR Serverless job is configured with default settings, which is suitable for most cases, e.g., hourly processing.</p> <p>If your data volume is enormous, e.g., total row counts of a batch exceed 100,000,000, the default settings may not fit this situation, which will cause the EMR job to fail. It would help if you changed the configuration of your EMR Spark job.</p> <p>You can configure the resources used by the EMR spark job by adding the file <code>s3://{PipelineS3Bucket}/{PipelineS3Prefix}{ProjectId}/config/spark-config.json</code> in the S3 bucket.</p> <p>Please replace <code>{PipelineS3Bucket}</code>, <code>{PipelineS3Prefix}</code>, and <code>{ProjectId}</code> with the values of your data pipeline. These values are found in the <code>Clickstream-DataProcessing-&lt;uuid&gt;</code> stack's Parameters.</p> <p>Also, you can get these values by running the below commands,</p> <pre><code>stackNames=$(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --no-paginate  | jq -r '.StackSummaries[].StackName' | grep  Clickstream-DataProcessing  | grep -v Nested)\necho -e \"$stackNames\" | while read stackName; do\naws cloudformation describe-stacks --stack-name $stackName  | jq '.Stacks[].Parameters' | jq 'map(select(.ParameterKey == \"PipelineS3Bucket\" or .ParameterKey == \"PipelineS3Prefix\" or .ParameterKey == \"ProjectId\"))'\ndone\n</code></pre> <p>The data processing job took about 25 minutes to process 600,000,000 rows (200,000,000 requests, 170G gzip data) in solution benchmark testing via below configuration:</p> <pre><code>{\n\"sparkConfig\": [\n\"spark.emr-serverless.executor.disk=200g\",\n\"spark.executor.instances=16\",\n\"spark.dynamicAllocation.initialExecutors=16\",\n\"spark.executor.memory=100g\",\n\"spark.executor.cores=16\",\n\"spark.network.timeout=10000000\",\n\"spark.executor.heartbeatInterval=10000000\",\n\"spark.shuffle.registration.timeout=120000\",\n\"spark.shuffle.registration.maxAttempts=5\",\n\"spark.shuffle.file.buffer=2m\",\n\"spark.shuffle.unsafe.file.output.buffer=1m\"\n],\n\"inputRePartitions\": 2000\n}\n</code></pre> <p>Please make sure your account has enough emr-serverless quotas, you can view the quotas via emr-serverless-quotas in region us-east-1. For more configurations, please refer to Spark job properties and application worker config.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/","title":"Configure transformation and enrichment plugins","text":"<p>There are two types of plugins: transformer or enrichment.  When choose plugins, you can only have one transformer and zero or multiple enrichment for a pipeline.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#built-in-plugins","title":"Built-in Plugins","text":"<p>Below plugins are provided by Clickstream Analytics on AWS.</p> Plugin name Type Description UAEnrichment enrichment User-agent enrichment, use <code>ua_parser</code> Java library to enrich <code>User-Agent</code> in the HTTP header to <code>ua_browser</code>,<code>ua_browser_version</code>,<code>ua_os</code>,<code>ua_os_version</code>,<code>ua_device</code> IpEnrichment enrichment IP address enrichment, use GeoLite2 data by MaxMind to enrich <code>IP</code> to <code>city</code>, <code>continent</code>, <code>country</code> <p>The UAEnrichment uses UA Parser to parse user-agent in Http header.</p> <p>The IpEnrichment plugin uses GeoLite2-City data created by MaxMind, available from https://www.maxmind.com.</p>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#custom-plugins","title":"Custom Plugins","text":"<p>You can add custom plugins to transform raw event data or enrich the data for your need.</p> <p>Note</p> <p>To add custom plugins, you must develop your own plugins firstly, see Develop Custom Plugins</p> <p>You can add your plugins by click Add Plugin button, which will open a new window, in which you can upload your plugins.</p> <ol> <li>Give the plugin Name and Description.</li> <li>Chose Plugin Type,</li> <li>Enrichment: Plugin to add fields into event data collected by SDK (both Clickstream SDK or third-party SDK)</li> <li> <p>Transformation: A plugin used to transform a third-party SDK\u2019s raw data into solution built-in schema</p> </li> <li> <p>Upload plugin java JAR file.</p> </li> <li> <p>(Optional) Upload the dependency files if any.</p> </li> <li> <p>Main function class: fill the full class name of your plugin class name, e.g. <code>com.example.sol.CustomTransformer</code>.</p> </li> </ol>"},{"location":"pipeline-mgmt/data-processing/configure-plugin/#develop-custom-plugins","title":"Develop Custom Plugins","text":"<p>The simplest way to develop custom plugins is making changes based on our example project.</p> <ol> <li>Clone/Fork the example project.</li> </ol> <pre><code>git clone https://github.com/awslabs/clickstream-analytics-on-aws.git\n\ncd examples/custom-plugins\n</code></pre> <ul> <li>For enrichment plugin, please refer to the example: <code>custom-enrich/</code></li> <li> <p>For transformer plugin, please refer to the example: <code>custom-sdk-transformer/</code></p> </li> <li> <p>Change packages and classes name as your desired.</p> </li> <li> <p>Implement the method <code>public Dataset&lt;row&gt; transform(Dataset&lt;row&gt; dataset)</code> to do transformation or enrichment.</p> </li> <li> <p>(Optional) Write test code.</p> </li> <li> <p>Run gradle to package code to jar <code>./gradlew clean build</code>.</p> </li> <li> <p>Get the jar file in build output directory <code>./build/libs/</code>.</p> </li> </ul>"},{"location":"pipeline-mgmt/data-processing/data-schema/","title":"Data schema","text":"<p>This article explains the data schema and format in Clickstream Analytics on AWS. This solution uses an event-based data model to store and analyze clickstream data, every activity (e.g., click, view) on the clients is modeled as an event with multiple dimensions. Dimensions are common for all events, but customers have the flexibility to use JSON object to store values into some dimensions (e.g., event parameters, user attributes), which will cater for the need of collecting information that are specific for their business. Those JSON will be stored in special data types which allow customers to unnest the values in the analytics engines.</p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#database-and-table","title":"Database and table","text":"<p>For each project, the solution creates a database with name of <code>&lt;project-id&gt;</code> in Redshift and Athena. In Redshift, each App will have a schema with name of <code>app_id</code>, within which event data are stored in <code>ods_event</code> tables, user-related attributes are stored in <code>dim_user</code> table. In Athena, data from all apps in the project will be stored in <code>ods_event</code> table with partitions of app_id, year, month, and day.</p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#columns","title":"Columns","text":"<p>Each column in the ods_event table represents an event-specific parameter. Note that some parameters are nested within a Super field in Redshift or Array in Athena, and those fields (e.g., items and event_params) contains parameters that are repeatable. Table columns are described below.</p>"},{"location":"pipeline-mgmt/data-processing/data-schema/#event-fields","title":"Event fields","text":"Field Name Data Type - Redshift Data Type - Athena Description event_id VARCHAR STRING Unique ID for the event. event_date DATE DATE The date when the event was logged (YYYYMMDD format in UTC). event_timestame BIGINT BIGINT The time (in microseconds, UTC) when the event was logged on the client. event_previous_timestamp BIGINT BIGINT The time (in microseconds, UTC) when the event was previously logged on the client. event_name VARCHAR STRING The name of the event. event_value_in_usd BIGINT BIGINT The currency-converted value (in USD) of the event's \"value\" parameter. event_bundle_sequence_id BIGINT BIGINT The sequential ID of the bundle in which these events were uploaded. ingest_timestamp BIGINT BIGINT Timestamp offset between collection time and upload time in micros."},{"location":"pipeline-mgmt/data-processing/data-schema/#event-parameter-fields","title":"Event parameter fields","text":"Field Name Data Type - Redshift Data Type - Athena Description event_params SUPER ARRAY Event parameters. event_params.key VARCHAR STRING The name of the event parameter. event_params.value SUPER ARRAY A record containing the event parameter's value. event_params.value.string_value VARCHAR STRING If the event parameter is represented by a string, such as a URL or campaign name, it is populated in this field. event_params.value.int_value BIGINT INTEGER If the event parameter is represented by an integer, it is populated in this field. event_params.value.double_value DOUBLE PRECISION FLOAT If the event parameter is represented by a double value, it is populated in this field. event_params.value.float_value DOUBLE PRECISION FLOAT If the event parameter is represented by a floating point value, it is populated in this field. This field is not currently in use."},{"location":"pipeline-mgmt/data-processing/data-schema/#user-fields","title":"User fields","text":"Field name Data type - Redshift Data type - Athena Description user_id VARCHAR STRING The unique ID assigned to a user through <code>setUserId()</code> API. user_pseudo_id VARCHAR STRING The pseudonymous id generated by SDK for the user. user_first_touch_timestamp BIGINT BIGINT The time (in microseconds) at which the user first opened the app or visited the site."},{"location":"pipeline-mgmt/data-processing/data-schema/#user-property-fields","title":"User property fields","text":"Field name Data type - RedShift Data type - Athena Description user_properties SUPER ARRAY Properties of the user. user_properties.key VARCHAR STRING The name of the user property. user_properties.value SUPER ARRAY A record for the user property value. user_properties.value.string_value VARCHAR STRING The string value of the user property. user_properties.value.int_value BIGINT BIGINT The integer value of the user property. user_properties.value.double_value DOUBLE PRECISION FLOAT The double value of the user property. user_properties.value.float_value DOUBLE PRECISION FLOAT This field is currently unused. user_properties.value.set_timestamp_micros BIGINT BIGINT The time (in microseconds) at which the user property was last set. user_ltv SUPER ARRAY The Lifetime Value of the user. user_ltv.revenue DOUBLE PRECISION FLOAT The Lifetime Value (revenue) of the user. user_ltv.revenue DOUBLE PRECISION FLOAT The Lifetime Value (currency) of the user."},{"location":"pipeline-mgmt/data-processing/data-schema/#device-fields","title":"Device fields","text":"Field name Data type Data type - Athena Description device.mobile_brand_name VARCHAR STRING The device brand name. device.mobile_model_name VARCHAR STRING The device model name. device.manufacturer VARCHAR STRING The device manufacturer name. device.carrier VARCHAR STRING The device network provider name. device.network_type VARCHAR STRING The network_type of the device, e.g., WIFI, 5G device.operating_system VARCHAR STRING The operating system of the device. device.operating_system_version VARCHAR STRING The OS version. device.vendor_id VARCHAR STRING IDFV (present only if IDFA is not collected). device.advertising_id VARCHAR STRING Advertising ID/IDFA. device.system_language VARCHAR STRING The OS language. device.time_zone_offset_seconds BIGINT BIGINT The offset from GMT in seconds. device.ua_browser VARCHAR STRING The browser in which the user viewed content, derived from User Agent string device.ua_browser_version VARCHAR STRING The version of the browser in which the user viewed content, derive from User Agent device.ua_device VARCHAR STRING The device in which user viewed content, derive from User Agent. device.ua_device_category VARCHAR STRING The device category in which user viewed content, derive from User Agent. device.screen_width VARCHAR STRING The screen width of the device. device.screen_height VARCHAR STRING The screen height of the device."},{"location":"pipeline-mgmt/data-processing/data-schema/#geo-fields","title":"Geo fields","text":"Field name Data type - Redshift Data type - Athena Description geo.continent VARCHAR STRING The continent from which events were reported, based on IP address. geo.sub_continent VARCHAR STRING The subcontinent from which events were reported, based on IP address. geo.country VARCHAR STRING The country from which events were reported, based on IP address. geo.region VARCHAR STRING The region from which events were reported, based on IP address. geo.metro VARCHAR STRING The metro from which events were reported, based on IP address. geo.city VARCHAR STRING The city from which events were reported, based on IP address. geo.locale VARCHAR STRING The locale information obtained from device."},{"location":"pipeline-mgmt/data-processing/data-schema/#traffic-fields","title":"Traffic fields","text":"Field name Data type - Redshift Data type - Athena Description traffic_source.name VARCHAR STRING Name of the marketing campaign that acquired the user when the events were reported. traffic_source.medium VARCHAR STRING Name of the medium (paid search, organic search, email, etc.) that  acquired the user when the events were reported. traffic_source.source VARCHAR STRING Name of the network source that acquired the user when the event were reported."},{"location":"pipeline-mgmt/data-processing/data-schema/#app-info-fields","title":"App Info fields","text":"Field name Data type - Redshift Data type - Athena Description app_info.id VARCHAR STRING The package name or bundle ID of the app. app_info.app_id VARCHAR STRING The App ID (created by this solution) associated with the app. app_info.install_source VARCHAR STRING The store that installed the app. app_info.version VARCHAR STRING The app's versionName (Android) or short bundle version."},{"location":"pipeline-mgmt/data-processing/data-schema/#other-fields","title":"Other fields","text":"Field name Data type - Redshift Data type - Athena Description platform VARCHAR STRING The data stream platform (Web, IOS or Android) from which the event originated. project_id VARCHAR STRING The project id associated with the app. items SUPER ARRAY Key-value records contain information about items associated with the event ecommerce SUPER ARRAY Key-value records contain information about ecommerce-specify attributes associated with the events event_dimensions SUPER ARRAY Key-value records contain information about additional dimensions associated with the events privacy_info SUPER ARRAY Key-value records contain information about user privacy setting associated with the events"},{"location":"pipeline-mgmt/ingestion/","title":"Data ingestion","text":"<p>Data ingestion module contains a web service that provides an endpoint to collect data through HTTP/HTTPS requests, which mainly is composed of Amazon Application Load Balancer and Amazon Elastic Container Service. It also supports sinking data into a stream service or S3 directly. </p> <p>You can create a data ingestion module with the following settings:</p> <ul> <li> <p>Ingestion endpoint settings: Create a web service as an ingestion endpoint to collect data sent from your SDKs.</p> </li> <li> <p>Data sink settings: Configure how the solution sinks the data for downstream consumption. Currently, the solution supports three types of data sink:</p> <ul> <li>Apache Kafka</li> <li>Amazon S3</li> <li>Amazon Kinesis Data Stream (KDS)</li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/configure-ingestion-endpoint/","title":"Ingestion endpoint settings","text":"<p>The solution creates a web service as an ingestion endpoint to collect data sent from your SDKs. You can set below configurations for ingestion endpoint.</p> <ul> <li> <p>Public Subnets: select at least two existing VPC public subnets, and the Amazon Application Load Balancers (ALBs) will be deployed in these subnets.</p> </li> <li> <p>Private Subnets: select at least two existing VPC private subnets, and the EC2 instances running in ECS will be deployed in these subnets.</p> <p>Tip</p> <p>The availability zones where the public subnets are located must be consistent with those of the private subnets.</p> </li> <li> <p>Ingestion capacity: This configuration sets the capacity of the ingestion server, and the ingestion server will automatically scale up or down based on the utilization of the processing CPU.</p> <ul> <li>Minimum capacity: The minimum capacity to which the ingestion server will scale down.</li> <li>Maximum capacity: The maximum capacity to which the ingestion server will scale up.</li> <li>Warm pool: Warm pool gives you the ability to decrease latency for your applications that have exceptionally long boot time. For more information, please refer to Warm pools for Amazon EC2 Auto Scaling.</li> </ul> </li> <li> <p>Enable HTTPS: Users can choose HTTPS/HTTP protocol for the Ingestion endpoint.</p> <ul> <li>Enable HTTPS: If users choose to enable HTTPS, the ingestion server will provide HTTPS endpoint. <ul> <li>Domain name: input a domain name. Once the ingestion server is created, use the custom endpoint to create an alias or CNAME mapping in your Domain Name System (DNS) for the custom endpoint. </li> <li>SSL Certificate: User need to select an ACM certificate corresponding to the domain name that you input. If there is no ACM certificate, please refer create public certificate to create it.</li> </ul> </li> <li> <p>Disable HTTPS: If users choose to disable HTTPS, the ingestion server will provide HTTP endpoint.</p> <p>Warning</p> <p>Using HTTP protocol is not secure, because data will be sent without any encryption, and there are high risks of data being leaked or tampered during transmission. Please acknowledge the risk to proceed.</p> </li> </ul> </li> <li> <p>Additional Settings</p> <ul> <li>Request path: User can input the path of ingestion endpoint to collect data, the default path is \"/collect\".</li> <li>AWS Global Accelerator: User can choose to create an accelerator to get static IP addresses that act as a global fixed entry point to your ingestion server, which will improves the availability and performance of your ingestion server.    Note That additional charges apply.</li> <li> <p>Authentication: User can use OIDC provider to authenticate the request sent to your ingestion server. If you plan to enable it, please create an OIDC client in the OIDC provider then create a secret in AWS Secret Manager with information:</p> <ul> <li>issuer</li> <li>token endpoint</li> <li>User endpoint</li> <li>Authorization endpoint</li> <li>App client ID</li> <li>App Client Secret</li> </ul> <p>The format is like: <pre><code>  {\n\"issuer\":\"xxx\",\n\"userEndpoint\":\"xxx\",\n\"authorizationEndpoint\":\"xxx\",\n\"tokenEndpoint\":\"xxx\",\n\"appClientId\":\"xxx\",\n\"appClientSecret\":\"xxx\"\n}\n</code></pre> Note: In the OIDC provider, you need to add <code>https://&lt;ingestion server endpoint&gt;/oauth2/idpresponse</code> to \"Allowed callback URLs\"</p> </li> <li> <p>Access logs: ALB supports delivering detailed logs of all requests it receives. If you enable this option, the solution will automatically enable access logs for you and store the logs into the S3 bucket you selected in previous step.</p> <p>Tip</p> <p>The bucket must have a bucket policy that grants Elastic Load Balancing permission to write to the bucket.</p> <p>Below is an example policy for the bucket in regions available before August 2022,</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::&lt;elb-account-id&gt;:root\"\n},\n\"Action\": \"s3:PutObject\",\n\"Resource\": \"arn:aws:s3:::&lt;BUCKET&gt;/clickstream/*\"\n}\n]\n}\n</code></pre> <p>Replace <code>elb-account-id</code> with the ID of the AWS account for Elastic Load Balancing for your Region:</p> <ul> <li>US East (N. Virginia) \u2013 127311923021</li> <li>US East (Ohio) \u2013 033677994240</li> <li>US West (N. California) \u2013 027434742980</li> <li>US West (Oregon) \u2013 797873946194</li> <li>Africa (Cape Town) \u2013 098369216593</li> <li>Asia Pacific (Hong Kong) \u2013 754344448648</li> <li>Asia Pacific (Jakarta) \u2013 589379963580</li> <li>Asia Pacific (Mumbai) \u2013 718504428378</li> <li>Asia Pacific (Osaka) \u2013 383597477331</li> <li>Asia Pacific (Seoul) \u2013 600734575887</li> <li>Asia Pacific (Singapore) \u2013 114774131450</li> <li>Asia Pacific (Sydney) \u2013 783225319266</li> <li>Asia Pacific (Tokyo) \u2013 582318560864</li> <li>Canada (Central) \u2013 985666609251</li> <li>Europe (Frankfurt) \u2013 054676820928</li> <li>Europe (Ireland) \u2013 156460612806</li> <li>Europe (London) \u2013 652711504416</li> <li>Europe (Milan) \u2013 635631232127</li> <li>Europe (Paris) \u2013 009996457667</li> <li>Europe (Stockholm) \u2013 897822967062</li> <li>Middle East (Bahrain) \u2013 076674570225</li> <li>South America (S\u00e3o Paulo) \u2013 507241528517</li> <li>China (Beijing) \u2013 638102146993</li> <li>China (Ningxia) \u2013 037604701340</li> </ul> </li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/","title":"Apache Kafka","text":"<p>This data sink will stream the clickstream data collected by the ingestion endpoint into an topic in a Kafka cluster. Currently, solution support Amazon Managed Streaming for Apache Kafka (Amazon MSK).</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#amazon-msk","title":"Amazon MSK","text":"<ul> <li> <p>Select an existing Amazon MSK cluster. Select an MSK cluster from the drop-down list, the MSK cluster needs to meet the following requirements:</p> <ul> <li>MSK cluster and this solution need to be in the same VPC</li> <li>Enable Unauthenticated access in Access control methods</li> <li>Enable Plaintext in Encryption</li> <li>Set auto.create.topics.enable as <code>true</code> in MSK cluster configuration. This configuration sets whether MSK cluster can create topic automatically. Or You need to create the specific topic in your Kafka cluster before creating the data pipeline.</li> <li>The value of default.replication.factor cannot be larger than the number of MKS cluster brokers</li> </ul> <p>Note: If there is no MSK cluster, the user needs to create an MSK Cluster follow above requirements.</p> </li> <li> <p>Topic: The user can specify a topic name. By default, the solution will create a topic with \"project-id\".</p> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#connector","title":"Connector","text":"<p>Enable solution to create Kafka connector and a custom plugin for this connector. This connector will sink the data from Kafka cluster to S3 bucket.</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kafka/#additional-settings","title":"Additional Settings:","text":"<ul> <li>Sink maximum interval: Specifies the maximum length of time (in seconds) that records should be buffered before streaming to the AWS service.</li> <li>Batch size: The maximum number of records to deliver in a single batch.</li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/","title":"Amazon Kinesis Data Stream","text":"<p>This data sink will stream the clickstream data collected by the ingestion endpoint into KDS. The solution will create a KDS in your AWS account based on your specifications.</p>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/#provision-mode","title":"Provision mode","text":"<p>Two modes are available: On-demand and Provisioned</p> <ul> <li> <p>On-demand: In this mode, KDS shards are provisioned based on the workshop automatically. On-demand mode is suited for workloads with unpredictable and highly-variable traffic patterns. </p> </li> <li> <p>Provisioned: In this mode, KDS shards are set at creation. The provisioned mode is suited for predictable traffic with capacity requirements that are easy to forecast. You can also use the provisioned mode if you want fine-grained control over how data is distributed across shards. </p> <ul> <li>Shard number: With the provisioned mode, you must specify the number of shards for the data stream.  For more information about shard, please refer to provisioned mode</li> </ul> </li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-kinesis/#addtional-settings","title":"Addtional Settings","text":"<ul> <li>Sink maximum interval: With this configuration, you can specify the maximum interval (in seconds) that records should be buffered before streaming to the AWS service.</li> <li>Batch size: With this configuration, you can specify the maximum number of records to deliver in a single batch.</li> </ul>"},{"location":"pipeline-mgmt/ingestion/create-data-sink-w-s3/","title":"Amazon S3","text":"<p>In this option, clickstream data is buffered in the memory of ingestion Server, then sink into a S3 bucket. This option provides the best cost-performance in case real-time data consumption is not required. </p> <p>Note</p> <p>Unlike Kafka and KDS data sink, this option buffers data in the ingestion server and responses 200 code to SDK client before sink into S3, so there is chance data could be lost while ingestion server fails and auto-scaled machine is in the process of creation. But it is worth to note that this probability is very low because of the High-availability design of the solution.</p> <ul> <li>S3 URI: You can select an amazon s3 bucket. The data will be sank into this bucket.</li> <li>S3 prefix: By default, the solution adds prefix of \"/YYYY/MM/dd/HH\" (in UTC) to the data files when delivered to S3. You can provide additional prefix which will be added before . <li>Buffer size: Specify the data size to buffer before sending to Amazon S3. The higher buffer size may be lower in cost with higher latency, the lower buffer size will be faster in delivery with higher cost. Min: 1 MiB, Max: 50 MiB</li> <li>Buffer interval: Specify the max interval (second) for saving buffer to S3. The higher interval allows more time to collect data and the size of data may be bigger. The lower interval sends the data more frequently and may be more advantageous when looking at shorter cycles of data activity. Min: 60 Second, Max: 3600 Second</li>"},{"location":"pipeline-mgmt/quicksight/configure-quicksight/","title":"Configure Reporting","text":""},{"location":"pipeline-mgmt/quicksight/configure-quicksight/#quicksight","title":"QuickSight","text":"<p>Once the data are processed and modeled by the data pipeline, the solution supports creating out-of-the-box dashboards in QuickSight.</p> <p>Note</p> <p>To enable this module, your AWS account needs to have subscription in QuickSight. If it hasn't, please follow this sign up for Amazon QuickSight to create a subscription first.</p> <p>You need to select a QuickSight User for the solution to use to create datasets and dashboards. If you don't have one, click Create a user button on the solution console, which will automatically create a user for you.</p>"},{"location":"plan-deployment/cost/","title":"Cost","text":"<p>Important</p> <p>The following cost estimations are examples and may vary depending on your environment.</p> <p>You are responsible for the cost of AWS services used when running this solution. Deploying this solution will only create a solution web console in your AWS account, which is completely serverless and typically can be covered within free tier.</p> <p>The majority of the cost for this solution is incurred by the data pipeline. As of this revision, the main factors affecting the solution cost include:</p> <ul> <li> <p>Ingestion module: the cost depends on the size of the ingestion server and the type of the data sink you choose.</p> </li> <li> <p>Data processing and modeling module (optional): the cost depends on whether you choose to enabled this module and its relevant configurations</p> </li> <li> <p>Enabled Dashboards (optional): the cost depends on whether you choose to enabled this module and its relevant configurations</p> </li> <li> <p>Additional features</p> </li> </ul> <p>The following are cost estimations for monthly data volumes of 10/100/1000 RPS (request per second) with different data pipeline configurations. Cost estimation are provided by modules. To get a total cost for your use case, sum the cost by modules based on your actual configuration.</p> <p>Important</p> <p>As of this revision, the following cost is calculated with <code>On-Demand</code> prices in <code>us-east-1</code> region measured in USD.</p>"},{"location":"plan-deployment/cost/#ingestion-module","title":"Ingestion module","text":"<p>Ingestion module includes the following cost components:</p> <ul> <li>Application load balancer</li> <li>EC2 for ECS</li> <li>Data sink (Kinesis Data Streams | Kafka | Direct to S3)</li> <li>S3 storage</li> </ul> <p>Key assumptions include:</p> <ul> <li>Request payload: 1KB (compressed, 1:10 ratio)</li> <li>MSK configurations (m5.large * 2)</li> <li>KDS configuration (on-demand, provision - shard 2)</li> <li>10/100/1000RPS</li> </ul> Request Per Second ALB EC2 Buffer type Buffer cost S3 Total (USD/Month) 10RPS 7 122 Kinesis (On-Demand) 36 3 168 7 122 Kinesis (Provisioned 2 shard) 22 3 154 7 122 MSK (m5.large * 2, connector MCU * 1) 417 3 549 7 122 None 3 132 100RPS 43 122 Kinesis(On-demand) 86 3 254 43 122 Kinesis (Provisioned 2 shard) 26 3 194 43 122 MSK (m5.large * 2, connector MCU * 1) 417 3 585 43 122 None 3 168 1000RPS 396 122 Kinesis(On-demand) 576 14 1108 396 122 Kinesis (Provisioned 10 shard) 146 14 678 396 122 MSK (m5.large * 2, connector MCU * 2~3) 530 14 1062 396 122 None 14 532"},{"location":"plan-deployment/cost/#data-processing-data-modeling-modules","title":"Data processing &amp; data modeling modules","text":"<p>Data processing &amp; modeling module include the following cost components if you choose to enable:</p> <ul> <li> <p>EMR Serverless</p> </li> <li> <p>Redshift</p> </li> </ul> <p>Key assumptions include:</p> <ul> <li>10/100/1000/10000 RPS</li> <li>Data processing interval: hourly/6-hourly/daily</li> <li>EMR running three built-in plugins to process data</li> </ul> Request Per Second EMR schedule interval EMR Cost Redshift type Redshift cost Total (USD) 10RPS Hourly 28 Serverless (8 based RPU) 68 96 6-hourly 10.8 Serverless(8 based RPU) 11 21.8 Daily 9.6 Serverless(8 based RPU) 3 12.6 100RPS Hourly 105 Serverless (8 based RPU) 72 177 6-hourly 99 Serverless(8 based RPU) 17.2 116.2 Daily 140 Serverless(8 based RPU) 16.9 156.9 1000RPS Hourly 1362 Serverless (8 based RPU) 172 1534 6-Hourly 678 Serverless (8 based RPU) 176 854 Daily 873 Serverless (8 based RPU) 352 1225 <p>Note</p> <p>For the cost of 1000 PRS Daily, we used below EMR configuration.</p> <pre><code>{\n\"sparkConfig\": [\n\"spark.emr-serverless.executor.disk=80g\",\n\"spark.executor.instances=8\",\n\"spark.dynamicAllocation.initialExecutors=16\",\n\"spark.executor.memory=80g\",\n\"spark.executor.cores=16\"\n],\n\"inputRePartitions\": 2000\n}\n</code></pre>"},{"location":"plan-deployment/cost/#reporting-module","title":"Reporting module","text":"<p>Reporting module include the following cost components if you choose to enable:</p> <ul> <li>QuickSight</li> </ul> <p>Key assumptions include</p> <ul> <li>QuickSight Enterprise subscription</li> <li>Exclude Q cost</li> <li>Two authors with monthly subscription</li> <li>Ten readers with 22 working days per month, 5% active readers, 50% frequent readers, 25%  occasional readers, 20% inactive readers</li> <li>10GB SPICE capacity</li> </ul> Daily data volume/RPS Authors Readers SPICE Total All size 48 18.80 0 66.80 <p>Note</p> <p>All your data pipelines are applied to the above QuickSight costs, even the visualizations managed outside the solution.</p>"},{"location":"plan-deployment/cost/#logs-and-monitoring","title":"Logs and Monitoring","text":"<p>The solution utilizes CloudWatch Logs\uff0c CloudWatch Metrics and CloudWatch Dashboard to implement logging, monitoring and visualizating features. The total cost is around $14 per month and may fluctuate based on the volume of logs and the number of metrics being monitored.</p>"},{"location":"plan-deployment/cost/#additional-features","title":"Additional features","text":"<p>You will be charged with additional cost only if you choose to enable the following features.</p>"},{"location":"plan-deployment/cost/#secrets-manager","title":"Secrets Manager","text":"<ul> <li> <p>If you enable reporting, the solution creates a secret in Secrets Manager to store the Redshift credentials used by QuickSight visualization. Cost: 0.4 USD/month.</p> </li> <li> <p>If you enable the authentication feature of the ingestion module, you need to create a secret in Secrets Manager to store the information for OIDC. Cost: 0.4 USD/month.</p> </li> </ul>"},{"location":"plan-deployment/cost/#amazon-global-accelerator","title":"Amazon Global Accelerator","text":"<p>It incurs a fixed hourly charge and a per-day volume data transfer cost.</p> <p>Key assumptions:</p> <ul> <li>Ingestion deployment in <code>us-east-1</code></li> </ul> Request Per Second Fixed hourly cost Data transfer cost Total cost(USD) 10RPS 18 0.3 18.3 100RPS 18 3 21 1000RPS 18 30 38"},{"location":"plan-deployment/cost/#application-load-balancer-access-log","title":"Application Load Balancer Access log","text":"<p>You are charged storage costs for Amazon S3, but not charged for the bandwidth used by Elastic Load Balancing to send log files to Amazon S3. For more information about storage costs, see\u00a0Amazon S3 pricing.</p> Request Per Second Log size(GB) S3 cost(USD) 10 RPS 16.5 0.38 100 RPS 165 3.8 1000 RPS 1650 38"},{"location":"plan-deployment/regions/","title":"Supported AWS Regions","text":""},{"location":"plan-deployment/regions/#regional-deployments","title":"Regional deployments","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List.</p> <p>Clickstream Analytics on AWS provides two types of authentication for its web console, Cognito User Pool and OpenID Connect (OIDC) Provider. You must choose to launch the solution with OpenID Connect in case one of the following scenarios:</p> <ul> <li>Cognito User Pool is not available in your AWS Region.</li> <li>You already have an OpenID Connect Provider and want to authenticate against it.</li> </ul> <p>Supported regions for web console deployment</p> Region Name Launch with Cognito User Pool Launch with OpenID Connect US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet China (Ningxia) Regions operated by NWCD <p>This solution provides modular components for supporting different data pipeline architecture. The data processing, data modeling and reporting modules are optional, that is, you can create a data pipeline without data processing, data modeling and reporting modules if needed.</p> <p>Pipeline modules availability</p> Region Name Data ingestion with MSK as buffer Data ingestion with KDS as buffer Data ingestion with S3 as buffer Data processing Data modeling with Redshift Serverless Data modeling with Provisioned Redshift Reporting with QuickSight US East (N. Virginia) US East (Ohio) US West (N. California) US West (Oregon) Africa (Cape Town) Asia Pacific (Hong Kong) Asia Pacific (Mumbai) Asia Pacific (Osaka) Asia Pacific (Seoul) Asia Pacific (Singapore) Asia Pacific (Sydney) Asia Pacific (Tokyo) Canada (Central) Europe (Frankfurt) Europe (Ireland) Europe (London) Europe (Milan) Europe (Paris) Europe (Stockholm) Middle East (Bahrain) South America (Sao Paulo) China (Beijing) Region Operated by Sinnet* China (Ningxia) Region Operated by NWCD* <p>Note(*)</p> <p>AWS China Regions don't support using AWS Global Accelerator to accelerate the ingestion endpoint.</p>"},{"location":"plan-deployment/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, see AWS Cloud Security.</p>"},{"location":"plan-deployment/security/#iam-roles","title":"IAM Roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s AWS Lambda functions, Amazon API Gateway and Amazon Cognito or OpenID connect access to create regional resources.</p>"},{"location":"plan-deployment/security/#amazon-vpc","title":"Amazon VPC","text":"<p>This solution optionally deploys a web console within your VPC. You can isolate access to the web console via Bastion hosts, VPNs, or Direct Connect. You can create VPC endpoints to let traffic between your Amazon VPC and AWS services not leave the Amazon network to satisfy the compliance requirements.</p>"},{"location":"plan-deployment/security/#security-groups","title":"Security Groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running.</p>"},{"location":"plan-deployment/security/#amazon-cloudfront","title":"Amazon CloudFront","text":"<p>This solution optionally deploys a web console hosted in an Amazon S3 bucket and Amazon API Gateway. To help reduce latency and improve security, this solution includes an Amazon CloudFront distribution with an Origin Access Control (OAC), which is a CloudFront user that provides public access to the solution\u2019s website bucket contents. For more information, refer to Restricting access to an Amazon S3 origin in the Amazon CloudFront Developer Guide.</p>"},{"location":"resources/upload-ssl-certificate/","title":"Upload SSL Certificate to IAM","text":"<p>Upload the SSL certificate by running the AWS CLI command <code>upload-server-certificate</code> similar to the following:</p> <pre><code>aws iam upload-server-certificate --path /cloudfront/ \\\n--server-certificate-name YourCertificate \\\n--certificate-body file://Certificate.pem \\\n--certificate-chain file://CertificateChain.pem \\\n--private-key file://PrivateKey.pem\n</code></pre> <p>Replace the file names and YourCertificate with the names for your uploaded files and certificate. You must specify the <code>file://</code> prefix in the certificate-body, certificate-chain and private-key parameters in the API request.  Otherwise, the request fails with a <code>MalformedCertificate: Unknown</code> error message.</p> <p>Note</p> <p>You must specify a path using the --path option. The path must begin with /cloudfront and must include a trailing slash (for example, /cloudfront/test/).</p> <p>After the certificate is uploaded, the AWS command <code>upload-server-certificate</code> returns metadata for the uploaded certificate, including the certificate's Amazon Resource Name (ARN), friendly name, identifier (ID), and expiration date.</p> <p>To view the uploaded certificate, run the AWS CLI command <code>list-server-certificates</code>:</p> <pre><code>aws iam list-server-certificates\n</code></pre> <p>For more information, see uploading a server certificate to IAM.</p>"},{"location":"sdk-manual/","title":"Overview","text":"<p>Clickstream Analytics on AWS provides different client-side SDKs, which can make it easier for you to report click stream data to the data pipeline created in the solution. Currently, the solution supports the following platforms:</p> <ul> <li>Android </li> <li>Swift </li> </ul>"},{"location":"sdk-manual/#key-features-and-benefits","title":"Key features and benefits","text":"<ul> <li>Automatic data collection. Clickstream SDKs provide built-in capabilities to automatically collect common events, such as screen view, session, and user engagement, so that you only need to focus on recording business-specific events.</li> <li>Ease of use. Clickstream SDKs provide multiple APIs and configuration options to simplify the event reporting and attribute setting operation.</li> <li>Cross-platform analytics. Clickstream SDKs are consistent in event data structure, attribute validation rules, and event sending mechanism, so that data can be normalized in the same structure for cross-platform analytics.</li> </ul> <p>Note</p> <p>All Clickstream SDKs are open source under Apache 2.0 License in GitHub. You can customize the SDKs if needed. All contributions are welcome.</p>"},{"location":"sdk-manual/android/","title":"Clickstream Android SDK","text":""},{"location":"sdk-manual/android/#introduction","title":"Introduction","text":"<p>Clickstream Android SDK can help you easily collect in-app click stream data from Android devices to your AWS environments through the data pipeline privisoned by this solution.</p> <p>The SDK is based on the Amplify for Android SDK Core Library and developed according to the Amplify Android SDK plug-in specification. In addition, the SDK provides features that automatically collect common user events and attributes (for example, screen view, and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/android/#platform-support","title":"Platform Support","text":"<p>Clickstream Android SDK supports Android 4.1 (API level 16) and later. </p>"},{"location":"sdk-manual/android/#integrate-the-sdk","title":"Integrate the SDK","text":""},{"location":"sdk-manual/android/#1-include-the-sdk","title":"1. Include the SDK","text":"<p>Add the following dependency to your <code>app</code> module's <code>build.gradle</code> file.</p> <pre><code>dependencies {\nimplementation 'software.aws.solution:clickstream:0.5.1'\n}\n</code></pre> <p>Next, synchronize your project with the latest version:  </p>"},{"location":"sdk-manual/android/#2-configure-parameters","title":"2. Configure parameters","text":"<p>Find the <code>res</code> directory under your <code>project/app/src/main</code>, and manually create a raw folder in the <code>res</code> directory. </p> <p> </p> <p>Download your <code>amplifyconfiguration.json</code> file from your clickstream control plane, and paste it to the raw folder. The JSON file is like:</p> <pre><code>{\n\"analytics\": {\n\"plugins\": {\n\"awsClickstreamPlugin\": {\n\"appId\": \"appId\",\n\"endpoint\": \"https://example.com/collect\",\n\"isCompressEvents\": true,\n\"autoFlushEventsInterval\": 10000,\n\"isTrackAppExceptionEvents\": false\n}\n}\n}\n}\n</code></pre> <p>In the file, your <code>appId</code> and <code>endpoint</code> are already configured. The explanation for each property is as follows:</p> <ul> <li>appId: the app id of your project in control plane.</li> <li>endpoint: the endpoint url you will upload the event to AWS server.</li> <li>isCompressEvents: whether to compress event content when uploading events, and the default value is <code>true</code></li> <li>autoFlushEventsInterval: event sending interval, and the default value is <code>10s</code></li> <li>isTrackAppExceptionEvents: whether auto track exception event in app, and the default value is <code>false</code></li> </ul>"},{"location":"sdk-manual/android/#3-initialize-the-sdk","title":"3. Initialize the SDK","text":"<p>Initialize the SDK in the application <code>onCreate()</code> method.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\npublic void onCreate() {\nsuper.onCreate();\ntry{\nClickstreamAnalytics.init(this);\nLog.i(\"MyApp\", \"Initialized ClickstreamAnalytics\");\n} catch (AmplifyException error){\nLog.e(\"MyApp\", \"Could not initialize ClickstreamAnalytics\", error);\n} }\n</code></pre>"},{"location":"sdk-manual/android/#4-configure-the-sdk","title":"4. Configure the SDK","text":"<p>After initializing the SDK, you can use the following code to customize it.</p> <p>Important</p> <p>This configuration will override the default configuration in <code>amplifyconfiguration.json</code> file.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\n// config the SDK after initialize.\nClickstreamAnalytics.getClickStreamConfiguration()\n.withAppId(\"appId\")\n.withEndpoint(\"https://example.com/collect\")\n.withAuthCookie(\"your authentication cookie\")\n.withSendEventsInterval(10000)\n.withSessionTimeoutDuration(1800000)\n.withTrackAppExceptionEvents(false)\n.withLogEvents(true)\n.withCustomDns(CustomOkhttpDns.getInstance())\n.withCompressEvents(true);\n</code></pre>"},{"location":"sdk-manual/android/#5-record-event","title":"5. Record event","text":"<p>Add the following code where you need to report an event. For more information, refer to Github.</p> <pre><code>import software.aws.solution.clickstream.ClickstreamAnalytics;\nimport software.aws.solution.clickstream.ClickstreamEvent;\nClickstreamEvent event = ClickstreamEvent.builder()\n.name(\"PasswordReset\")\n.add(\"Channel\", \"SMS\")\n.add(\"Successful\", true)\n.add(\"ProcessDuration\", 78.2)\n.add(\"UserAge\", 20)\n.build();\nClickstreamAnalytics.recordEvent(event);\n// for record an event directly\nClickstreamAnalytics.recordEvent(\"button_click\");\n</code></pre>"},{"location":"sdk-manual/android/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/android/#data-types","title":"Data types","text":"<p>Clickstream Android SDK supports the following data types:</p> Data type Range Example int -2147483648 \uff5e 2147483647 12 long -9223372036854775808 \uff5e 9223372036854775807 26854775808 double 4.9E-324 \uff5e 1.7976931348623157E308 3.14 boolean true, false true String max 1024\u00a0characters \"Clickstream\""},{"location":"sdk-manual/android/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contain uppercase and lowercase letters, numbers, and underscores. In case of an invalid event name, it will throw <code>IllegalArgumentException</code>. In case of an invalid attribute name or user attribute name, it will discard the attribute and record error.</p> </li> <li> <p>Do not use <code>_</code> as prefix in an event name or attribute name, because the <code>_</code> prefix is reserved for the solution.</p> </li> <li> <p>The event name and attribute name are case sensitive, so <code>Add_to_cart</code> and <code>add_to_cart</code> will be recognized as two different event names.</p> </li> </ol>"},{"location":"sdk-manual/android/#event-and-attribute-limitation","title":"Event and attribute limitation","text":"<p>In order to improve the efficiency of querying and analysis, we apply limits to event data as follows:</p> Name Recommended Maximum Handle strategy for\u00a0exceed Length of event name less than 25 characters 50 characters throw IllegalArgumentException Length of event attribute name less than 25 characters 50 characters discard, log and\u00a0record error Length of event attribute value less than 100 characters 1024 characters discard, log and\u00a0record error Event attribute per event less than 50 attributes 500 event attribute discard, log and\u00a0record error User attribute number less than 25 attributes 100 user attributes discard, log and\u00a0record error Length of\u00a0user attribute name less than 25 characters 50\u00a0characters discard, log and\u00a0record error Length of\u00a0user attribute value less than 50 characters 256 characters discard, log and\u00a0record error <p>Important</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include common attributes and preset attributes.</li> <li>If the attribute or user attribute with the same name is added more than twice, the latest value will apply.</li> </ul>"},{"location":"sdk-manual/android/#preset-events-and-attributes","title":"Preset events and attributes","text":""},{"location":"sdk-manual/android/#preset-events","title":"Preset events","text":"<p>Automatically collected events:</p> Event name Triggered Event Attributes _session_start when users app come to foreground for the first time and there is no on-going session _session_id\u00a0_session_start_timestamp_session_duration _screen_view when the activity callback <code>onResume()</code> method _screen_name_screen_id_previous_screen_name_previous_screen_id_entrances_engagement_time_msec _app_exception when the app crashes _exception_message_exception_stack _app_update when the app is updated to a new version and launched again _previous_app_version _first_open when the user launches an app the first time after installation _os_update when device operating system is updated to a new version _previous_os_version _user_engagement when the app is in the foreground for at least one second _engagement_time_msec _profile_set when the <code>addUserAttributes()</code>\u00a0or <code>setUserId()</code>\u00a0API is called"},{"location":"sdk-manual/android/#session-definition","title":"Session definition","text":"<p>In Clickstream Android SDK, we do not limit the total time of a session. As long as the time between the next entry of the app and the last exit time is within the allowable timeout period, the current session is considered to be continuous.</p> <ul> <li> <p>_session_start: When the app starts for the first time, or the app was launched to the foreground and the time between the last exit exceeded <code>session_time_out</code> period.</p> </li> <li> <p>_session_duration: We calculate the <code>_session_duration</code> by minus the current event create timestamp and the session's <code>_session_start_timestamp</code>. This attribute will be added in every event during the session.</p> </li> <li> <p>session_time_out: By default, it is 30 minutes, which can be customized through the configuration API.</p> </li> <li> <p>_session_number: The total number of sessions by distinct session id, and <code>_session_number</code> will be appear in every event's attribute object.</p> </li> </ul>"},{"location":"sdk-manual/android/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream Android SDK, we define the <code>user_engagement</code> as the app which is in the foreground for at least one second.</p> <ul> <li> <p>when to send: We send the event when the app navigate to background or navigate to another app.</p> </li> <li> <p>engagement_time_msec: We count the time from when the app comes in the foreground to when the app goes to the background.</p> </li> </ul>"},{"location":"sdk-manual/android/#common-attributes-and-reserved-attributes","title":"Common attributes and reserved attributes","text":""},{"location":"sdk-manual/android/#sample-event-structure","title":"Sample event structure","text":"<pre><code>{\n\"hashCode\": \"80452b0\",\n\"unique_id\": \"c84ad28d-16a8-4af4-a331-f34cdc7a7a18\",\n\"event_type\": \"PasswordReset\",\n\"event_id\": \"460daa08-0717-4385-8f2e-acb5bd019ee7\",\n\"timestamp\": 1667877566697,\n\"device_id\": \"f24bec657ea8eff7\",\n\"platform\": \"Android\",\n\"os_version\": \"10\",\n\"make\": \"HUAWEI\",\n\"brand\":\"HUAWEI\",\n\"model\": \"TAS-AN00\",\n\"locale\": \"zh_CN_#Hans\",\n\"carrier\": \"CDMA\",\n\"network_type\": \"Mobile\",\n\"screen_height\": 2259,\n\"screen_width\": 1080,\n\"zone_offset\": 28800000,\n\"system_language\": \"zh\",\n\"country_code\": \"CN\",\n\"sdk_version\": \"0.2.0\",\n\"sdk_name\": \"aws-solution-clickstream-sdk\",\n\"app_version\": \"1.0\",\n\"app_package_name\": \"com.notepad.app\",\n\"app_title\": \"Notepad\",\n\"app_id\": \"notepad-4a929eb9\",\n\"user\": {\n\"_user_id\": {\n\"value\":\"312121\",\n\"set_timestamp\": 1667877566697\n},\n\"_user_name\": {\n\"value\":\"carl\",\n\"set_timestamp\": 1667877566697\n},\n\"_user_first_touch_timestamp\": {\n\"value\":1667877267895,\n\"set_timestamp\": 1667877566697\n}\n},\n\"attributes\": {\n\"Channel\": \"SMS\",\n\"Successful\": true,\n\"Price\": 120.1,\n\"ProcessDuration\": 791,\n\"_session_id\":\"dc7a7a18-20221108-031926703\",\n\"_session_start_timestamp\": 1667877566703,\n\"_session_duration\": 391809,\n\"_session_number\": 1\n}\n}\n</code></pre> <p>All user attributes will be stored in <code>user</code> object, and all custom and global attributes in <code>attributes</code> object.</p>"},{"location":"sdk-manual/android/#common-attribute","title":"Common attribute","text":"attribute describe how to generate use and purpose hashCode the event object's hash code generate from <code>Integer.toHexString(AnalyticsEvent.hashCode())</code> distinguish different event app_id clickstream app id generated when clickstream app create from solution control plane. identify the events for your apps unique_id the unique id for user generate from <code>UUID.randomUUID().toString()</code> when the sdk first initialization it will be changed after user relogin to another user who never login, and when user relogin to the before user in same device, the unique_id\u00a0will reset to the before user's\u00a0unique_id. the unique for identity different user and\u00a0associating the behavior of logging in and not logging in device_id the unique id for device generate from <code>Settings.System.getString(context.getContentResolver(), Settings.Secure.ANDROID_ID)</code>, if Android ID is null or \"\", we will use UUID instead. distinguish different device event_type event name set by developer or SDK. distinguish different event type event_id the qniue id for event generate from\u00a0<code>UUID.randomUUID().toString()</code>\u00a0when the event create. distinguish each event timestamp event create timestamp generate from\u00a0<code>System.currentTimeMillis()</code>\u00a0when event create data analysis needs platform the platform name for Android device is always \"Android\" data analysis needs os_version the platform version code generate from <code>Build.VERSION.RELEASE</code> data analysis needs make manufacturer of the device generate from <code>Build.MANUFACTURER</code> data analysis needs brand brand of\u00a0the device generate from <code>Build.BRAND</code> data analysis needs model model of the device generate from <code>Build.MODEL</code> data analysis needs carrier the device network operator name <code>TelephonyManager.getNetworkOperatorName()</code>default is: \"UNKNOWN\" data analysis needs network_type the current device network type \"Mobile\", \"WIFI\" or \"UNKNOWN\"generate from\u00a0<code>android.netConnectivityManager</code> data analysis needs screen_height The absolute height of the available display size in pixels generate from <code>applicationContext.resources.displayMetrics.heightPixels</code> data analysis needs screen_width The absolute width of the available display size in pixels. generate from <code>applicationContext.resources.displayMetrics.widthPixels</code> data analysis needs zone_offset the divece raw offset from GMT in milliseconds. generate from <code>java.util.Calendar.get(Calendar.ZONE_OFFSET)</code> data analysis needs locale the default locale(language, country and variant) for this device of the Java Virtual Machine generate from\u00a0<code>java.util.Local.getDefault()</code> data analysis needs system_language the devie language code generate from\u00a0<code>java.util.Local.getLanguage()</code>default is: \"UNKNOWN\" data analysis needs country_code country/region code for this device generate from\u00a0<code>java.util.Local.getCountry()</code>default is: \"UNKNOWN\" data analysis needs sdk_version clickstream sdk version generate from <code>BuildConfig.VERSION_NAME</code> data analysis needs sdk_name clickstream sdk name this will always be\u00a0\"aws-solution-clickstream-sdk\" data analysis needs app_version the app version name of user's app. generate from <code>android.content.pm.PackageInfo.versionName</code>default is: \"UNKNOWN\" data analysis needs app_package_name the app package name\u00a0 of user's app. generate from <code>android.content.pm.PackageInfo.packageName</code>default is: \"UNKNOWN\" data analysis needs app_title the app's display name generate from <code>android.content.pm.getApplicationLabel(appInfo)</code> data analysis needs"},{"location":"sdk-manual/android/#reserved-attributes","title":"Reserved attributes","text":"<p>User attributes</p> Attribute name Description _user_id Reserved for user id that is assigned by app _user_ltv_revenue Reserved for user lifetime value _user_ltv_currency Reserved for user lifetime value currency _user_first_touch_timestamp The time (in microseconds) when the user first opened the app or visited the site, and it is included in every event in <code>user</code>\u00a0object <p>Event attributes</p> Attribute name Description _traffic_source_medium Reserved for traffic medium. Use this attribute to store the medium that acquired user when events were logged. _traffic_source_name Reserved for traffic name. Use this attribute to store the marketing campaign that acquired user when events were logged. _traffic_source_source Reserved for traffic source.  Name of the network source that acquired the user when the event were reported. _channel The channel for app was downloaded _device_vendor_id Vendor id of the device _device_advertising_id Advertising id of the device _entrances Added in <code>_screen_view</code>\u00a0event. The first <code>_screen_view</code> event in a session has the value 1, and others 0. _session_id Added in all events. _session_start_timestamp Added in all events. _session_duration Added in all events. _session_number Added in all events. The initial value is 1, and the value is automatically incremented by user device."},{"location":"sdk-manual/swift/","title":"Clickstream Swift SDK","text":""},{"location":"sdk-manual/swift/#introduction","title":"Introduction","text":"<p>Clickstream Swift SDK can help you easily collect in-app click stream data from iOS devices to your AWS environments through the data pipeline provisioned by this solution.</p> <p>The SDK is based on the Amplify for Swift SDK Core Library and developed according to the Amplify Swift SDK plug-in specification. In addition, the SDK provides features that automatically collect common user events and attributes (for example, screen view, and first open) to accelerate data collection for users.</p>"},{"location":"sdk-manual/swift/#platform-support","title":"Platform Support","text":"<p>Clickstream Swift SDK supports iOS 13+.</p> <p>API Documentation</p>"},{"location":"sdk-manual/swift/#integrate-sdk","title":"Integrate SDK","text":"<p>Clickstream requires Xcode 13.4 or higher to build.</p>"},{"location":"sdk-manual/swift/#1add-package","title":"1.Add Package","text":"<p>We use Swift Package Manager to distribute Clickstream Swift SDK, open your project in Xcode and select File &gt; Add Packages.</p> <p></p> <p>Enter the Clickstream Library for Swift GitHub repo URL (<code>https://github.com/awslabs/clickstream-swift</code>) into the search bar, You'll see the Clickstream Library for Swift repository rules for which version of Clickstream you want Swift Package Manager to install. Choose\u00a0Up to Next Major Version, then click\u00a0Add Package, make the Clickstream product checked as default, and click Add Package again.</p> <p></p>"},{"location":"sdk-manual/swift/#2parameter-configuration","title":"2.Parameter configuration","text":"<p>Download your\u00a0<code>amplifyconfiguration.json</code>\u00a0file from your Clickstream solution control plane, and paste it to your project root folder:</p> <p></p> <p>the json file will be as follows:</p> <pre><code>{\n\"analytics\": {\n\"plugins\": {\n\"awsClickstreamPlugin \": {\n\"appId\": \"appId\",\n\"endpoint\": \"https://example.com/collect\",\n\"isCompressEvents\": true,\n\"autoFlushEventsInterval\": 10000,\n\"isTrackAppExceptionEvents\": false\n}\n}\n}\n}\n</code></pre> <p>Your <code>appId</code> and <code>endpoint</code> are already set up in it, here's an explanation of each property:</p> <ul> <li>appId: the app id of your project in control plane.</li> <li>endpoint: the endpoint url you will upload the event to AWS server.</li> <li>isCompressEvents: whether to compress event content when uploading events, default is <code>true</code></li> <li>autoFlushEventsInterval: event sending interval, the default is <code>10s</code></li> <li>isTrackAppExceptionEvents: whether auto track exception event in app, default is <code>false</code></li> </ul>"},{"location":"sdk-manual/swift/#3initialize-the-sdk","title":"3.Initialize the SDK","text":"<p>Once you have configured the parameters, you need to initialize it in AppDelegate's <code>didFinishLaunchingWithOptions</code> lifecycle method to use the SDK.</p> <pre><code>import Clickstream\n...\nfunc application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -&gt; Bool {\n// Override point for customization after application launch.\ndo {\ntry ClickstreamAnalytics.initSDK()\n} catch {\nassertionFailure(\"Fail to initialize ClickstreamAnalytics: \\(error)\")\n}\nreturn true\n}\n</code></pre>"},{"location":"sdk-manual/swift/#4config-the-sdk","title":"4.Config the SDK","text":"<pre><code>import Clickstream\n// config the sdk after initialize.\ndo {\nvar configuration = try ClickstreamAnalytics.getClickstreamConfiguration()\nconfiguration.appId = \"appId\"\nconfiguration.endpoint = \"https://example.com/collect\"\nconfiguration.authCookie = \"your authentication cookie\"\nconfiguration.sessionTimeoutDuration = 1800000\nconfiguration.isLogEvents = true\nconfiguration.isCompressEvents = true    \nconfiguration.isLogEvents = true\n} catch {\nprint(\"Failed to config ClickstreamAnalytics: \\(error)\")\n}\n</code></pre> <p>Note: this configuation will override the default configuation in <code>amplifyconfiguration.json</code>\u00a0file</p>"},{"location":"sdk-manual/swift/#5record-event","title":"5.Record event","text":"<p>Add the following code where you need to report an event.</p> <pre><code>import Clickstream\nlet attributes: ClickstreamAttribute = [\n\"channel\": \"apple\",\n\"uccessful\": true,\n\"ProcessDuration\": 12.33,\n\"UserAge\": 20,\n]\nClickstreamAnalytics.recordEvent(eventName: \"testEvent\", attributes: attributes)\n// for record an event directly\nClickstreamAnalytics.recordEvent(eventName: \"button_click\")\n</code></pre> <p>For more usage refer to Github start using</p> <p>For Objective-c project refer to ClickstreamObjc Api Reference</p>"},{"location":"sdk-manual/swift/#data-format-definition","title":"Data format definition","text":""},{"location":"sdk-manual/swift/#data-type","title":"Data type","text":"<p>Clickstream Swift SDK supports the following data types:</p> Data type Range Sample Int -2147483648\uff5e2147483647 12 Int64 -9,223,372,036,854,775,808\uff5e 9,223,372,036,854,775,807 26854775808 Double -2.22E-308~1.79E+308 3.14 Boolean true, false true String max support 1024\u00a0characters \"clickstream\""},{"location":"sdk-manual/swift/#naming-rules","title":"Naming rules","text":"<ol> <li> <p>The event name and attribute name cannot start with a number, and only contains uppercase and lowercase letters, numbers, underscores, if the event name is invalid will throw <code>precondition failure</code>, if the attribute or user attribute name is invalid the attribute will discard and record error.</p> </li> <li> <p>Do not use <code>_</code> as prefix to naming event name and attribute name, <code>_</code> prefix is the reserved from Clickstream Analytics.</p> </li> <li> <p>The event name and attribute name are in case sensitive, So the event <code>Add_to_cart</code> and <code>add_to_cart</code> will be Recognized as two different event.</p> </li> </ol>"},{"location":"sdk-manual/swift/#event-attribute-limitation","title":"Event &amp; Attribute Limitation","text":"<p>In order to improve the efficiency of querying and analysis, we need to limit events as follows:</p> Name Suggestion Hard limit Handle strategy for\u00a0exceed Length of event name under 25 character 50 character throw error Length of event attribute name under 25 character 50 character discard, log and\u00a0record error Length of event attribute value under 100 character 1024 character discard, log and\u00a0record error Event attribute per event under 50 attribute 500 event attribute discard, log and\u00a0record error User attribute number under 25 attribute 100 user attribute discard, log and\u00a0record error Length of\u00a0User attribute name under 25 character 50\u00a0character discard, log and\u00a0record error Length of\u00a0User attribute value under 50 character 256 character discard, log and\u00a0record error <p>Explanation of limits</p> <ul> <li>The character limits are the same for single-width character languages (e.g., English) and double-width character languages (e.g., Chinese).</li> <li>The limit of event attribute per event include common attributes and preset attributes.</li> <li>If you add the same name of attribute or user attribute more than twice, the last value applies.</li> </ul>"},{"location":"sdk-manual/swift/#preset-events-and-attributes","title":"Preset events and attributes","text":""},{"location":"sdk-manual/swift/#preset-events","title":"Preset events","text":"<p>Automatically collected events:</p> Event name When to trigger Event Attributes _session_start when users app come to foreground for the first time and their is no on-going session _session_id\u00a0_session_start_timestamp_session_duration _screen_view when the activity callback <code>viewDidAppear()</code> method _screen_name_screen_id_previous_screen_name_previous_screen_id_entrances_engagement_time_msec _app_exception when the app is crash. _exception_message_exception_stack _app_update when the app is updated to a new version and launched again _previous_app_version _first_open the first time user launches an app after installing _os_update device operating system is updated to a new version _previous_os_version _user_engagement when the app is in the foreground at least one second _engagement_time_msec _profile_set when the <code>addUserAttributes()</code>\u00a0or <code>setUserId()</code>\u00a0api called."},{"location":"sdk-manual/swift/#session-definition","title":"Session definition","text":"<p>In Clickstream Swift SDK, we do not limit the total time of a session, as long as the time between the next entry of the app and the last exit time is within the allowable timeout period, we consider the current session to be continuous.</p> <ul> <li> <p>_session_start: When the app starts for the first time, or the app was launched to the foreground and the time between the last exit exceeded <code>session_time_out</code> period.</p> </li> <li> <p>_session_duration: We calculate the <code>_session_duration</code> by minus the current event create timestamp and the session's <code>_session_start_timestamp</code>, this attribute will be added in every event during the session.</p> </li> <li> <p>session_time_out: By default is 30 minutes, which can be customized through the configuration api.</p> </li> <li> <p>_session_number: The total number of session distinct by session id, and <code>_session_number</code> will be appear in every event's attribute object.</p> </li> </ul>"},{"location":"sdk-manual/swift/#user-engagement-definition","title":"User engagement definition","text":"<p>In Clickstream Swift SDK, we define the <code>user_engagement</code> as the app is in the foreground at least one second.</p> <ul> <li> <p>when to send: We send the event when the app navigate to background or navigate to another app.</p> </li> <li> <p>engagement_time_msec: We count the time from when the app comes in the foreground to when the app goes to the background..</p> </li> </ul>"},{"location":"sdk-manual/swift/#common-attributes-and-reserved-attributes","title":"Common attributes and Reserved attributes","text":""},{"location":"sdk-manual/swift/#event-structure-sample","title":"Event structure sample","text":"<pre><code>{\n\"app_id\": \"Shopping\",\n\"app_package_name\": \"com.compny.app\",\n\"app_title\": \"ModerneShopping\",\n\"app_version\": \"1.0\",\n\"brand\": \"apple\",\n\"carrier\": \"UNKNOWN\",\n\"country_code\": \"US\",\n\"device_id\": \"A536A563-65BD-49BE-A6EC-6F3CE7AC8FBE\",\n\"device_unique_id\": \"\",\n\"event_id\": \"91DA4BBE-933F-4DFA-A489-8AEFBC7A06D8\",\n\"event_type\": \"add_to_cart\",\n\"hashCode\": \"63D7991D\",\n\"locale\": \"en_US (current)\",\n\"make\": \"apple\",\n\"model\": \"iPhone 14 Pro\",\n\"network_type\": \"WIFI\",\n\"os_version\": \"16.4\",\n\"platform\": \"iOS\",\n\"screen_height\": 2556,\n\"screen_width\": 1179,\n\"sdk_name\": \"aws-solution-clickstream-sdk\",\n\"sdk_version\": \"0.4.1\",\n\"system_language\": \"en\",\n\"timestamp\": 1685082174195,\n\"unique_id\": \"0E6614B7-2D2C-4774-AB2F-B0A9E6C3BFAC\",\n\"zone_offset\": 28800000,\n\"user\": {\n\"_user_city\": {\n\"set_timestamp\": 1685006678437,\n\"value\": \"Shanghai\"\n},\n\"_user_first_touch_timestamp\": {\n\"set_timestamp\": 1685006678434,\n\"value\": 1685006678432\n},\n\"_user_name\": {\n\"set_timestamp\": 1685006678437,\n\"value\": \"carl\"\n}\n},\n\"attributes\": {\n\"_session_duration\": 15349,\n\"_session_id\": \"0E6614B7-20230526-062238846\",\n\"_session_number\": 3,\n\"_session_start_timestamp\": 1685082158847,\n\"product_category\": \"men's clothing\",\n\"product_id\": 1\n}\n}\n</code></pre> <p>All user attributes will be included in <code>user</code> object, and all custom and global attribute are stored in <code>attributes</code> object.</p>"},{"location":"sdk-manual/swift/#common-attribute","title":"Common attribute","text":"attribute describe how to generate use and purpose hashCode the AnalyticsEvent Object's hashCode generate from<code>String(format: \"%08X\", hasher.combine(eventjson))</code> distinguish different event app_id clickstream app id generated when clickstream app create from solution control plane. identify the events for your apps unique_id the unique id for user generate from <code>UUID().uuidString</code> when the sdk first initializationit will be changed after user relogin to another user, and when user relogin to the before user in same device, the <code>unique_id</code>\u00a0will reset to the before user's\u00a0<code>unique_id</code>. the unique for identity different user and\u00a0associating the behavior of logging in and not logging in device_id the unique id for device generate from<code>UIDevice.current.identifierForVendor?.uuidString ?? UUID().uuidString</code>it will be changed after app reinstall distinguish different device device_unique_id the device advertising Id generate from<code>ASIdentifierManager.shared().advertisingIdentifier.uuidString ?? \"\"</code> distinguish different device event_type event name set by user or sdk. distinguish different event type event_id the unique id for event generate from\u00a0<code>UUID().uuidString</code>\u00a0when the event create. distinguish each event timestamp event create timestamp generate from\u00a0<code>Date().timeIntervalSince1970 * 1000</code>\u00a0when event create data analysis needs platform the platform name for iOS device is always \"iOS\" data analysis needs os_version the iOS os version generate from<code>UIDevice.current.systemVersion</code> data analysis needs make manufacturer of the device for iOS device is always \"apple\" data analysis needs brand brand of the device for iOS device is always \"apple\" data analysis needs model model of the device generate from mapping of device identifier data analysis needs carrier the device network operator name generate from<code>CTTelephonyNetworkInfo().serviceSubscriberCellularProviders?.first?.value</code>default is: \"UNKNOWN\" data analysis needs network_type the current device network type \"Mobile\", \"WIFI\" or \"UNKNOWN\"generate by\u00a0\u00a0<code>NWPathMonitor</code> data analysis needs screen_height The absolute height of the available display size in pixels generate from<code>UIScreen.main.bounds.size.height * UIScreen.main.scale</code> data analysis needs screen_width The absolute width of the available display size in pixels. generate from<code>UIScreen.main.bounds.size.width * UIScreen.main.scale</code> data analysis needs zone_offset the divece raw offset from GMT in milliseconds. generate from<code>TimeZone.current.secondsFromGMT()*1000</code> data analysis needs locale the default locale(language, country and variant) for this device generate from\u00a0<code>Locale.current</code> data analysis needs system_language the devie language code generate from\u00a0<code>Locale.current.languageCode</code>default is: \"UNKNOWN\" data analysis needs country_code country/region code for this device generate from\u00a0<code>Locale.current.regionCode</code>default is: \"UNKNOWN\" data analysis needs sdk_version clickstream sdk version generate from<code>PackageInfo.version</code> data analysis needs sdk_name clickstream sdk name this will always be <code>aws-solution-clickstream-sdk</code> data analysis needs app_version the app version name generate from\u00a0<code>Bundle.main.infoDictionary[\"CFBundleShortVersionString\"] ?? \"\"</code> data analysis needs app_package_name the app package name generate from<code>Bundle.main.infoDictionary[\"CFBundleIdentifier\"] ?? \"\"</code> data analysis needs app_title the app's display name generate from\u00a0<code>Bundle.main.infoDictionary[\"CFBundleName\"] ?? \"\"</code> data analysis needs"},{"location":"sdk-manual/swift/#reserved-attributes","title":"Reserved attributes","text":"<p>User attributes</p> attribute name description _user_id Reserved for user id that is assigned by app _user_ltv_revenue Reserved for user lifetime value _user_ltv_currency Reserved for user lifetime value currency _user_first_touch_timestamp The time (in microseconds) at which the user first opened the app or visited the site, it is included in every event in <code>user</code>\u00a0object <p>Event attributes</p> attribute name description _traffic_source_medium Reserved for traffic medium, use this attribute to store the medium that acquired user when events were logged. _traffic_source_name Reserved for traffic name, use this attribute to store the marketing campaign that acquired user when events were logged. _traffic_source_source Reserved for traffic source,  Name of the network source that acquired the user when the event were reported. _channel the channel for app was downloaded _device_vendor_id Vendor id of the device _device_advertising_id Advertising id of the device _entrances added in <code>_screen_view</code>\u00a0event , the first <code>_screen_view</code> event in a session the value is 1, others is 0. _session_id added in all event. _session_start_timestamp added in all event. _session_duration added in all event. _session_number added in all event, the initial value is 1, automatically increment by user device."},{"location":"solution-overview/","title":"Overview","text":"<p>The Clickstream Analytics on AWS solution allows you to collect, ingest, process and analyze click stream data from websites and mobile applications to drive your business growth. You can use the solution to create an analytics platform that fits your organizational needs, and maintain complete ownership and control over the valuable user behavior data. The solution can be applied to various use cases such as user behavior analysis and marketing analysis to improve website and application's performance.</p> <p>The solution provides modularized and configurable components of a data pipeline so that you can accelerate the building of a Well-Architected data pipeline from weeks to minutes. The purpose-built SDKs and guidance allow you to collect click stream data from different application platforms (for example, Android, iOS, and JavaScript) to AWS. In addition, the solution provides ready-to-use metrics and visualization templates enable you to derive actionable business insights easily and quickly.</p> <p> </p> <p>Use this navigation table to quickly find answers to these questions:</p> If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know which AWS Regions are supported for this solution Supported AWS Regions Get started with the solution quickly to build an end-to-end data pipeline, send data into the pipeline, and then view the out-of-the-box dashboards Getting Started Learn the concepts related to pipeline, and how to manage a data pipeline throughout its lifecycle Pipeline Management <p>The guide is intended for IT architects, data engineers, developers, DevOps, and data product managers with practical experience architecting in the AWS Cloud.</p>"},{"location":"solution-overview/features-and-benefits/","title":"Features and benefits","text":"<p>The solution includes the following key features:</p> <ul> <li>Visual data pipeline builder. You can simply define the data pipeline from a web-based UI console. The solution will take care of the underlying infrastructure creation, required security setup, and data integrations. Each pipeline module is built with various features and designed to be loosely-coupled, making it flexible for you to customize for specific use cases. </li> <li>Purposed-built SDKs. The SDKs are optimized for collecting data from Android, iOS, and JavaScript platforms, which automatically collect common events (for example, first visit, screen view), support built-in local cache, retry, and verification mechanisms to ensure high completeness of data transmission.</li> <li>Out-of-the-box dashboard. The solution offers built-in visualizations (for example, acquisition, engagement, retention, and user demographic) and exploratory reporting templates (for example, user details, event details), powering various critical business analytics use cases such as user behavior analytics, marketing analytics, and product analytics.</li> </ul>"},{"location":"solution-overview/use-cases/","title":"Use cases","text":"<p>Clickstream data play a pivotal role in numerous online business analytics use cases, and the Clickstream Analytics on AWS can be applied to the following: </p> <ul> <li>User behavior analytics: Clickstream data in user behavior analytics provides insights into the sequential and chronological patterns of user interactions on a website or application, helping businesses understand user navigation, preferences, and engagement levels to enhance the overall product experience and drive product innovation.</li> <li>First-party customer data platform (CDP): Clickstream data, together with other business data sources (for example, order history, and user profile), allow customers to create a first-party customer data platform that offers a comprehensive view of their users, enabling businesses to personalize customer experiences, optimize customer journeys, and deliver targeted marketing messages.</li> <li>Marketing analytics: Clickstream data in marketing analytics offers detailed information about users' click paths and interactions with marketing campaigns, enabling businesses to measure campaign effectiveness, optimize marketing strategies, and enhance conversion rates.</li> </ul>"}]}